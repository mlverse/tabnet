[{"path":"/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2020 RStudio, PBC Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"/articles/Hierarchical_classification.html","id":"data-preparation","dir":"Articles","previous_headings":"","what":"Data preparation","title":"Hierarchical Classification","text":"supported data format hierarchical classification Node object format package data.tree. general purpose format fits generic hierarchical tree encoding needs. node tree associated predictor values attributes data Node object. basic example acme dataset show two predictors values cost p associates attributes node hierarchy : Multiple manual programmatic methods available create update predictors. detailled vignette(\"data.tree\", package = \"data.tree\"). lot native hierarchical data-format conversion files Node covered thedata.tree package. can find “Create tree file” section vignette. needed, ape package covers lot conversion format philo format. Thus can reach Node format maybe two transformation steps… quick way achieve Node format data frame columns different levels hierarchy consist pasting columns single string \"/\" separator pathString column. turn expected hierarchy data.tree::.Node() command. Let’s starwars dataset toy example : may noticed name height unexpected values according original data: Human part name orginal dataset, height values changed local height tree. due rules follow create Node data frame.","code":"data(acme, package = \"data.tree\") acme$attributesAll print(acme, \"cost\", \"p\" , limit = 8) data(starwars, package = \"dplyr\") head(starwars, 4)  # erroneous Node construction starwars_tree <- starwars %>%    mutate(pathString = paste(\"StarWars_characters\", species, sex, `name`, sep = \"/\")) %>%   as.Node() print(starwars_tree, \"name\",\"height\", \"mass\", \"eye_color\", limit = 8)"},{"path":[]},{"path":"/articles/Hierarchical_classification.html","id":"avoid-factor-predictors","dir":"Articles","previous_headings":"Node preparation rules for {tabnet} models","what":"Avoid factor predictors","title":"Hierarchical Classification","text":".Node() consider .numeric() values factor(), turn characters applying .Node() function order {tabnet} properly embed .","code":""},{"path":"/articles/Hierarchical_classification.html","id":"avoid-column-name-collision-with-reserved-data-tree-names","dir":"Articles","previous_headings":"Node preparation rules for {tabnet} models","what":"Avoid column name collision with reserved {data.tree} names","title":"Hierarchical Classification","text":"name height part NODE_RESERVED_NAMES_CONST reserved list names Node attributes. must used predictor names, .Node() function silently discard .","code":""},{"path":"/articles/Hierarchical_classification.html","id":"avoid-column-named-level_-to-avoid-collision-with-output-data-tree-names","dir":"Articles","previous_headings":"Node preparation rules for {tabnet} models","what":"Avoid column named level_* to avoid collision with output data.tree names","title":"Hierarchical Classification","text":"dataset hierarchy turn internally multi-outcomes named level_1 level_n, n beeing depth tree. Thus column names starting level_ avoided.","code":""},{"path":"/articles/Hierarchical_classification.html","id":"ensure-the-last-hierarchy-of-the-tree-is-the-observation-id","dir":"Articles","previous_headings":"Node preparation rules for {tabnet} models","what":"Ensure the last hierarchy of the tree is the observation id","title":"Hierarchical Classification","text":"tree keeps single row attributes per tree leaf. Thus order transfer complete predictors dataset Node object, must keep last level hierarchy unique observation identifier (last resort beeing rowid_to_column() achieve ). classification done removing last level hierarchy case.","code":""},{"path":"/articles/Hierarchical_classification.html","id":"ensure-there-is-a-root-level-in-the-hierarchy","dir":"Articles","previous_headings":"Node preparation rules for {tabnet} models","what":"Ensure there is a root level in the hierarchy","title":"Hierarchical Classification","text":"tree single root nodes consistent. Thus use constant prefix pathString. classification done removing first level hierarchy case. Now let’s rules applied starwars_tree : can see reserved name column contains slightly different content original _name column.","code":"# demonstration of reserved column modification in Node construction starwars_tree <- starwars %>%    rename(`_name` = \"name\", `_height` = \"height\") %>%    mutate(pathString = paste(\"StarWars_characters\", species, sex, `_name`, sep = \"/\")) %>%   as.Node() print(starwars_tree, \"name\", \"_name\",\"_height\", \"mass\", \"eye_color\", limit = 8)"},{"path":[]},{"path":"/articles/Hierarchical_classification.html","id":"data-set-split","dir":"Articles","previous_headings":"Model building","what":"Data set split","title":"Hierarchical Classification","text":"starwars dataset contains list columns, hosting variability predictor values. Thus decide unnest_longer every list column values. triple size starwars dataset. dataset split done upfront transformation .Node(). use rsample::initial_split() split stratification parent category first level hierarchy species. order train model properly, prevent outcomes part predictor columns. sake demonstration, _name column present starwars_tree must now dropped. Now can see none predictor leaks outcome hierarchy information.","code":"starw_split <- starwars %>%    tidyr::unnest_longer(films) %>%    tidyr::unnest_longer(vehicles, keep_empty = TRUE) %>%    tidyr::unnest_longer(starships, keep_empty = TRUE) %>%    initial_split( prop = .8, strata = \"species\") # correct Node construction for hierarchical modeling starwars_train_tree <- starw_split %>%    training() %>%    # avoid reserved column names   rename(`_name` = \"name\", `_height` = \"height\") %>%    rowid_to_column() %>%    mutate(pathString = paste(\"StarWars_characters\", species, sex, rowid, sep = \"/\")) %>%   # remove outcomes labels from predictors   select(-species, -sex, -`_name`, -rowid) %>%    # turn it as hierarchical Node   as.Node()  starwars_test_tree <- starw_split %>%    testing() %>%    rename(`_name` = \"name\", `_height` = \"height\") %>%    rowid_to_column() %>%    mutate(pathString = paste(\"StarWars_characters\", species, sex, rowid, sep = \"/\")) %>%   select(-species, -sex, -`_name`, -rowid) %>%    as.Node()  starwars_train_tree$attributesAll"},{"path":"/articles/Hierarchical_classification.html","id":"model-building-1","dir":"Articles","previous_headings":"","what":"Model building","title":"Hierarchical Classification","text":"starwars_tree can now used input tabnet_fit() :","code":"config <- tabnet_config(decision_width = 8, attention_width = 8, num_steps = 3, penalty = .003, cat_emb_dim = 2, valid_split = 0.2, learn_rate = 1e-3, lr_scheduler = \"reduce_on_plateau\", early_stopping_monitor = \"valid_loss\", early_stopping_patience = 4, verbose = FALSE)  starw_model <- tabnet_fit(starwars_train_tree, config = config, epoch = 170, checkpoint_epochs = 25)"},{"path":"/articles/Hierarchical_classification.html","id":"model-diagnostic","dir":"Articles","previous_headings":"","what":"Model diagnostic","title":"Hierarchical Classification","text":"avoid verbose output model, thus first diagnostic check model -fitting though training loss plot. global feature importance gives us clue model quality","code":"autoplot(starw_model) vip::vip(starw_model)"},{"path":"/articles/Hierarchical_classification.html","id":"model-inference","dir":"Articles","previous_headings":"","what":"Model inference","title":"Hierarchical Classification","text":"can infer test-set can see Warnings dataset challenge many new levels found lot predictors test set. model also poor level_2 ( species ) level_3 ( sex ) definitively model-intended dataset. reason input dataset collecting large samples distinctive observation per leaf, rather diverse limited number characters compatible watching movie saga. Despite performance, local feature importance complete dataset : Hopefully hierarchical outcome better success one starwars dataset. journey, learned lot data format constraints solutions, now new performing solution toolbox.","code":"starwars_hat <- bind_cols(     predict(starw_model, starwars_test_tree),     node_to_df(starwars_test_tree)$y   ) tail(starwars_hat, n = 5) starwars_explain <- tabnet_explain(starw_model, starwars_test_tree) autoplot(starwars_explain) autoplot(starwars_explain, type = \"steps\")"},{"path":"/articles/Missing_data_predictors.html","id":"motivation","dir":"Articles","previous_headings":"","what":"Motivation","title":"Training a Tabnet model from missing-values dataset","text":"Real-life training dataset usually contains missing data. vast majority deep-learning networks handle missing data thus either stop crash values missing predictors. Tabnet use masking mechanism can reuse cover missing data training set. enter world missing-data, question type missing-data deal . missing data random (MAR), like example transmission errors sensor data dataset, missing random (MNAR) interactions exists missing data predictors values sample. later complex topic cover, try investigate ames dataset.","code":""},{"path":[]},{"path":"/articles/Missing_data_predictors.html","id":"ames-missings-understanding","dir":"Articles","previous_headings":"Missing-data dataset creation","what":"Ames missings understanding","title":"Training a Tabnet model from missing-values dataset","text":"ames dataset modeldata contains lot null values human analysis clearly understand implicit “missing object” described value. example pool surface 0 square meters means “pool”, basement surface 0 square meters means “basement”, … Many variables can detected visually inspecting distribution values like, example, Masonry veneer area predictor : know extremely difficult model capture internal representation distribution, thus want avoid null values penalize model internal representation.","code":"library(tidymodels, quietly = TRUE) library(tabnet) data(\"ames\", package = \"modeldata\") qplot(ames$Mas_Vnr_Area)"},{"path":"/articles/Missing_data_predictors.html","id":"while-keeping-some-room-for-freedom","dir":"Articles","previous_headings":"Missing-data dataset creation","what":"While keeping some room for freedom","title":"Training a Tabnet model from missing-values dataset","text":"Many variables come pair ames dataset, one qualitative aspect, quantitative aspect. example Pool_QC pool condition, “no_pool” level Pool_Area=0 case. human, intuition knowing pool present important modeling task. want model get internal representation implicit has_pool=FALSE without explicit dataset. order , let model freedom infer “no_pool” state thus mutate variables pair Pool_Area=NA Pool_QC=NA time.","code":""},{"path":"/articles/Missing_data_predictors.html","id":"ames-with-missing-data","dir":"Articles","previous_headings":"Missing-data dataset creation","what":"Ames with missing data","title":"Training a Tabnet model from missing-values dataset","text":"Let’s turn missing objects data explicitly NAs new ames_missing dataset : quick dirty way achieve numerical predictors na_if() zeros column name related surface area. , according keep room freedom rule, carefully matching categorical predictors can see variable missing random, thus can expect model capture missingness relation pretraining phase. Note: better way achieve proper value mutation explicit NAs also check qualitative column pair refers none zero occurrence equipment. beyond scope vignette.","code":"col_with_zero_as_na <- ames %>%    select(where(is.numeric)) %>%    select(matches(\"_SF|Area|Misc_Val|[Pp]orch$\")) %>%    summarise_each(min) %>%    select_if(~.x==0) %>%    names() ames_missing <- ames %>% mutate_at(col_with_zero_as_na, na_if, 0) %>%    mutate_at(\"Alley\", na_if, \"No_Alley_Access\") %>%    mutate_at(\"Fence\", na_if, \"No_Fence\") %>%    mutate_at(c(\"Garage_Cond\", \"Garage_Finish\"), na_if, \"No_Garage\") %>%    mutate_at(c(\"Bsmt_Exposure\", \"BsmtFin_Type_1\", \"BsmtFin_Type_2\"), na_if, \"No_Basement\")  visdat::vis_miss(ames_missing)"},{"path":"/articles/Missing_data_predictors.html","id":"model-pretraining","dir":"Articles","previous_headings":"","what":"Model pretraining","title":"Training a Tabnet model from missing-values dataset","text":"Let’s pretrain one model dataset, analyze variable importance emerge unsupervised representation learning step:","code":""},{"path":"/articles/Missing_data_predictors.html","id":"variable-importance-with-raw-ames-dataset","dir":"Articles","previous_headings":"Model pretraining","what":"Variable importance with raw ames dataset","title":"Training a Tabnet model from missing-values dataset","text":"Now capture columns missings, create convenience function color vip::vip() plot output according missingness quality column get BsmtFin_Type_1, BsmtFin_SF_1 Bsmt_Exposure variables top ten important variables according run pretraining model. variables screened missing values. Note result varies lot run run. captured result depends lot initialization conditions.","code":"ames_rec <- recipe(Sale_Price ~ ., data=ames) %>%    step_normalize(all_numeric())  cat_emb_dim <- map_dbl(ames %>% select_if(is.factor), ~log2(nlevels(.x)) %>% round)  ames_pretrain <- tabnet_pretrain(ames_rec, data=ames,  epoch=50, cat_emb_dim = cat_emb_dim,                             valid_split = 0.2, verbose=TRUE, batch=2930,                              early_stopping_patience = 3L, early_stopping_tolerance = 1e-4) autoplot(ames_pretrain) [Epoch 001] Loss: 43.708794 Valid loss: 8066126.500000 [Epoch 002] Loss: 31.463089 Valid loss: 5631984.000000 [Epoch 003] Loss: 23.396217 Valid loss: 3901085.500000 [Epoch 004] Loss: 19.241619 Valid loss: 2947481.750000 [Epoch 005] Loss: 15.032537 Valid loss: 2250338.000000 [Epoch 006] Loss: 12.991020 Valid loss: 1815583.125000 [Epoch 007] Loss: 11.044646 Valid loss: 1533597.875000 [Epoch 008] Loss: 9.114124 Valid loss: 1395840.000000 [Epoch 009] Loss: 8.362211 Valid loss: 1258169.375000 [Epoch 010] Loss: 7.549719 Valid loss: 1064599.500000 [Epoch 011] Loss: 6.808529 Valid loss: 998335.625000 [Epoch 012] Loss: 6.569450 Valid loss: 1047418.500000 [Epoch 013] Loss: 6.606429 Valid loss: 1048583.625000 [Epoch 014] Loss: 6.742617 Valid loss: 993241.312500 [Epoch 015] Loss: 6.806847 Valid loss: 995705.875000 [Epoch 016] Loss: 6.618536 Valid loss: 1026789.625000 [Epoch 017] Loss: 6.593469 Valid loss: 1033726.437500 Early stopping at epoch 017 col_with_missings <- ames_missing %>%   summarise_all(~sum(is.na(.)) > 0) %>%   t %>% enframe(name = \"Variable\") %>%    rename(has_missing = \"value\")  vip_color <- function(object, col_has_missing) {   vip_data <- vip::vip(object)$data %>% arrange(Importance)   vis_miss_plus <- left_join(vip_data, col_has_missing , by = \"Variable\") %>%     mutate(Variable = factor(Variable, levels = vip_data$Variable))   vis_miss_plus   ggplot(vis_miss_plus, aes(x = Variable, y = Importance, fill = has_missing)) +     geom_col() + coord_flip() + scale_fill_grey() } vip_color(ames_pretrain, col_with_missings)"},{"path":"/articles/Missing_data_predictors.html","id":"variable-importance-with-ames_missing-dataset","dir":"Articles","previous_headings":"Model pretraining","what":"Variable importance with ames_missing dataset","title":"Training a Tabnet model from missing-values dataset","text":"Let’s pretrain new model hyperparameter, now using ames_missing dataset. order compensate 13% missingness already present ames_missing dataset, adjust pretraining_ratio parameter 0.5 - 0.13 = 0.37  can see variables high missingness present top 10 important variables. seems good sign model captured proper interactions variables.","code":"ames_missing_rec <- recipe(Sale_Price ~ ., data = ames_missing) %>%    step_normalize(all_numeric()) ames_missing_pretrain <- tabnet_pretrain(ames_missing_rec, data = ames_missing, epoch = 50,                                      cat_emb_dim = cat_emb_dim,                                     valid_split = 0.2, verbose = TRUE, batch = 2930,                                      pretraining_ratio = 0.37,                                      early_stopping_patience = 3L, early_stopping_tolerance = 1e-4) autoplot(ames_missing_pretrain) vip_color(ames_missing_pretrain, col_with_missings) [Epoch 001] Loss: 56.250610 Valid loss: 40321308.000000 [Epoch 002] Loss: 44.254524 Valid loss: 39138240.000000 [Epoch 003] Loss: 33.992207 Valid loss: 38648800.000000 [Epoch 004] Loss: 26.421488 Valid loss: 37445656.000000 [Epoch 005] Loss: 22.290133 Valid loss: 35814052.000000 ... [Epoch 021] Loss: 10.877335 Valid loss: 20903176.000000 [Epoch 022] Loss: 11.023649 Valid loss: 20772972.000000 [Epoch 023] Loss: 10.819239 Valid loss: 20642806.000000 [Epoch 024] Loss: 10.994371 Valid loss: 20575458.000000 [Epoch 025] Loss: 10.700000 Valid loss: 20449918.000000 [Epoch 026] Loss: 10.902529 Valid loss: 20680102.000000 [Epoch 027] Loss: 10.791571 Valid loss: 20849496.000000 [Epoch 028] Loss: 11.102308 Valid loss: 20995910.000000 Early stopping at epoch 028"},{"path":[]},{"path":"/articles/Missing_data_predictors.html","id":"variable-importance-with-raw-ames-dataset-1","dir":"Articles","previous_headings":"Model training","what":"Variable importance with raw ames dataset","title":"Training a Tabnet model from missing-values dataset","text":", model uses two predictors BasmFin_SF_2 Garage_Finish respectively 88 % 5 % missingness.","code":"ames_fit <- tabnet_fit(ames_rec, data = ames,  tabnet_model = ames_pretrain,                              epoch = 50, cat_emb_dim = cat_emb_dim,                             valid_split = 0.2, verbose = TRUE, batch = 2930,                              early_stopping_patience = 5L, early_stopping_tolerance = 1e-4) autoplot(ames_fit) vip_color(ames_fit, col_with_missings)"},{"path":"/articles/Missing_data_predictors.html","id":"variable-importance-with-ames_missing-dataset-1","dir":"Articles","previous_headings":"Model training","what":"Variable importance with ames_missing dataset","title":"Training a Tabnet model from missing-values dataset","text":"can see one predictors Garage_Area 5 % missingness top 10.","code":"ames_missing_fit <- tabnet_fit(ames_rec, data = ames_missing,  tabnet_model = ames_missing_pretrain,                              epoch = 50, cat_emb_dim = cat_emb_dim,                             valid_split = 0.2, verbose = TRUE, batch = 2930,                              early_stopping_patience = 5L, early_stopping_tolerance = 1e-4) autoplot(ames_missing_fit) vip_color(ames_missing_fit, col_with_missings)"},{"path":"/articles/Missing_data_predictors.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Training a Tabnet model from missing-values dataset","text":"Even models huge variability variable importance among different training, intuition model trained explicit missing data provide better result counterpart trained zero-imputed variables. case, capability pretrain fit TabNet models MAR dataset MNAR dataset high convenience real-life use-cases.","code":""},{"path":"/articles/aum_loss.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Using ROC AUM loss for imbalanced binary classification","text":"previously used lending_club dataset highly imbalanced, leading challenging result binary classification task. Despite got fairly good accuracy default model design, roc_auc() metric poor, mainly due imbalanced problem. , see tabnet features allow improved performance family classification problems.","code":""},{"path":"/articles/aum_loss.html","id":"how-imbalance-is-my-problem","dir":"Articles","previous_headings":"","what":"How imbalance is my problem ?","title":"Using ROC AUM loss for imbalanced binary classification","text":"target variable Class imbalance can evaluated class imbalance Ratio : class_ratio 18.1, target variable seriously imbalanced, making minority class much harder model.","code":"class_ratio <- lending_club |>    summarise(sum( Class == \"good\") / sum( Class == \"bad\")) |>    pull()   class_ratio #> [1] 18.06576"},{"path":"/articles/aum_loss.html","id":"solutions-to-improve-imbalanced-classification-models","dir":"Articles","previous_headings":"","what":"Solutions to improve imbalanced classification models","title":"Using ROC AUM loss for imbalanced binary classification","text":"First, usual solution problem -sampling minority class, /-sampling majority class training data. won’t cover . second solution case weighting. {tidymodels} offers framework manage case weighting, ’ll first use compare two model families - XGBoost Tabnet - feature. Last, like also optimize model according metric looking . metric choice imbalanced dataset roc_auc() roc_pr(), definitively want loss function proxy . loss available {tabnet} nn_aum_loss() Optimizing ROC Curves Sort-Based Surrogate Loss Binary Classification Changepoint Detection (J Hillman, TD Hocking).","code":""},{"path":"/articles/aum_loss.html","id":"using-the-auc-metric-and-pr_curve-plots","dir":"Articles","previous_headings":"","what":"Using the AUC metric and pr_curve() plots","title":"Using ROC AUM loss for imbalanced binary classification","text":"Measuring ROC_AUC AUC_PR can’t separated plotting pr_curve(). Let’s baseline models two different workflows, one tabnet, XGBoost. big chunk code, mainly copy previous vignette. details noticed compute tag importance_weight() new column case_wts lending_club dataset. column excluded recipe() predictors role. explicitly mention column role workflow() via add_case_weights(). can now fit() model plot precision-recall curve test-set : Tabnet, case-weight, default loss XGBoost, case-weight models returning poor results.","code":"lending_club <- lending_club |>   mutate(     case_wts = if_else(Class == \"bad\", class_ratio, 1),     case_wts = importance_weights(case_wts)   )  split <- initial_split(lending_club, strata = Class) train <- training(split) test  <- testing(split)  tab_rec <- train |>   recipe() |>   update_role(Class, new_role = \"outcome\") |>   update_role(-has_role(c(\"outcome\", \"id\", \"case_weights\")), new_role = \"predictor\")  xgb_rec <- tab_rec |>    step_dummy(term, sub_grade, addr_state, verification_status, emp_length)  tab_mod <- tabnet(epochs = 100) |>    set_engine(\"torch\", device = \"cpu\") |>    set_mode(\"classification\")  xgb_mod <- boost_tree(trees = 100) |>    set_engine(\"xgboost\") |>    set_mode(\"classification\")  tab_wf <- workflow() |>    add_model(tab_mod) |>    add_recipe(tab_rec) |>    add_case_weights(case_wts)  xgb_wf <- workflow() |>    add_model(xgb_mod) |>    add_recipe(xgb_rec) |>    add_case_weights(case_wts) tab_fit <- tab_wf |> fit(train) xgb_fit <- xgb_wf |> fit(train)  tab_test <- tab_fit |> augment(test) xgb_test <- xgb_fit |> augment(test)  tab_test |>    pr_curve(Class, .pred_good) |>    autoplot() xgb_test |>   pr_curve(Class, .pred_good) |>   autoplot()"},{"path":"/articles/aum_loss.html","id":"case-weight","dir":"Articles","previous_headings":"","what":"Case-weight","title":"Using ROC AUM loss for imbalanced binary classification","text":"Weighting observation importance weight class made available {tabnet} marking one variable importance weight variable via workflow::add_case_weights() using case_weight variable inference time case_weights = parameter functions allows . Let’s proceed Tabnet, case-weight, default loss XGBoost, case-weight boost pr_curve() impressive models, Tabnet remains behind XGBoost here1.","code":"tab_test |>    pr_curve(Class, .pred_good, case_weights = case_wts) |>    autoplot() xgb_test |>   pr_curve(Class, .pred_good, case_weights = case_wts) |>   autoplot()"},{"path":"/articles/aum_loss.html","id":"roc_aum-loss","dir":"Articles","previous_headings":"","what":"ROC_AUM loss","title":"Using ROC AUM loss for imbalanced binary classification","text":"{tabnet} implement ROC AUM loss drive torch optimizer best possible AUC. Let’s use compare previous models : Now let’s compare result PR curve default loss side side: Tabnet, case-weight, default loss Tabnet, case-weight, ROC_AUM loss can see real2 improvement AUM loss, compared default nn_bce_loss() globally still poor recall.","code":"# configure the AUM loss tab_aum_mod <- tabnet(epochs = 100, loss = tabnet::nn_aum_loss, learn_rate = 0.02) |>    set_engine(\"torch\", device = \"cpu\") |>    set_mode(\"classification\")  # derive a workflow tab_aum_wf <- workflow() |>    add_model(tab_aum_mod) |>    add_recipe(tab_rec) |>    add_case_weights(case_wts)  # fit and augment the test dataset with prediction tab_aum_fit <- tab_aum_wf |> fit(train) tab_aum_test <- tab_aum_fit |> augment(test) tab_test |>    pr_curve(Class, .pred_good) |>    autoplot() tab_aum_test |>    pr_curve(Class, .pred_good) |>    autoplot()"},{"path":"/articles/aum_loss.html","id":"all-together","dir":"Articles","previous_headings":"","what":"All together","title":"Using ROC AUM loss for imbalanced binary classification","text":"Nothing prevent us use features, independent. . Moreover, without additional computation, done post inference. Tabnet, case-weight, default loss Tabnet, case-weight, ROC_AUM loss boost recall impressive, making Tabnet model far experimented challenger model3.","code":"tab_test |>    pr_curve(Class, .pred_good, case_weights = case_wts) |>    autoplot() tab_aum_test |>    pr_curve(Class, .pred_good, case_weights = case_wts) |>    autoplot()"},{"path":"/articles/interpretation.html","id":"experiments","dir":"Articles","previous_headings":"","what":"Experiments","title":"Interpretation tools","text":"show use interpretation tools tabnet, going perform 2 experiments using synthetic datasets similar used paper.","code":""},{"path":"/articles/interpretation.html","id":"datasets","dir":"Articles","previous_headings":"Experiments","what":"Datasets","title":"Interpretation tools","text":"First, let’s define functions use generate data: make_syn2 generate dataset 10 columns, columns 3-6 used calculate y response vector. similar Syn2 paper. make_syn4 generate dataset 10 columns . response vector depends column 10: value greater 0, use columns 1-2 compute logits, otherwise use columns 5-6. Now let’s generate datasets:","code":"logit_to_y <- function(logits) {   p <- exp(logits)/(1 + exp(logits))   y <- factor(ifelse(p > 0.5, \"yes\", \"no\"), levels = c(\"yes\", \"no\"))   y }  make_random_x <- function(n) {   x <- as.data.frame(lapply(1:10, function(x) rnorm(n)))   names(x) <- sprintf(\"V%02d\", 1:10)   x }  make_syn2 <- function(n = 5000) {   x <- make_random_x(n)   logits <- rowSums(x[,3:6])   x$y <- logit_to_y(logits)   x }  make_syn4 <- function(n = 5000) {   x <- make_random_x(n)   logits <- ifelse(     x[,10] > 0,     rowSums(x[,1:2]),     rowSums(x[,5:6])   )      x$y <- logit_to_y(logits)   x } syn2 <- make_syn2() syn4 <- make_syn4()"},{"path":"/articles/interpretation.html","id":"syn2","dir":"Articles","previous_headings":"Experiments","what":"Syn2","title":"Interpretation tools","text":"Let’s fit TabNet model syn2 dataset analyze interpretation metrics. feature importance plot can see , expected, features V03-V06 far important ones.  Now let’s visualize aggregated-masks plot. figure see observation x axis variable y axis. colors represent importance feature predicting value observation.  can see region V03 V06 concentrates higher intensity colors, variables close 0. expected variables considered building dataset. Next, can visualize attention masks step architecture.  see first step captures lot noise, 2 steps focus specifically important features.","code":"fit_syn2 <- tabnet_fit(y ~ ., syn2, epochs = 45, learn_rate = 0.06, device = \"cpu\") vip::vip(fit_syn2) library(tidyverse) ex_syn2 <- tabnet_explain(fit_syn2, syn2)  autoplot(ex_syn2, quantile = 0.99) autoplot(ex_syn2, type=\"steps\")"},{"path":"/articles/interpretation.html","id":"syn-4","dir":"Articles","previous_headings":"","what":"Syn 4","title":"Interpretation tools","text":"Now let’s analyze results Syn4 dataset. dataset little complicated TabNet ’s strong interaction variables. Depending V10, different variables used create response variable expect see masks. First fit model 10 epochs. feature importance plot , expected, strong importance V10, features used conditionally - either V01-V02 V05-V06.  Now let’s visualize attention masks. Notice arranged dataset V10 can easily visualize interaction effects. also trimmed 98th percentile colors shows importance even strong outliers.  figure see V10 important observations. also see first half dataset V05 V06 important feature, half, V01 V02 important ones. can also visualize masks step architecture.  see step 1 3 focus V10, different additional features, depending V10. Step 2 seems found noise V08, also focuses strongly V01-V02 V05-V06.","code":"fit_syn4 <- tabnet_fit(y ~ ., syn4, epochs = 50, device = \"cpu\", learn_rate = 0.08) vip::vip(fit_syn4) ex_syn4 <- tabnet_explain(fit_syn4, arrange(syn4, V10))  autoplot(ex_syn4, quantile=.98) autoplot(ex_syn4, type=\"steps\", quantile=.995)"},{"path":"/articles/selfsupervised_training.html","id":"data-preprocessing","dir":"Articles","previous_headings":"","what":"Data preprocessing","title":"Self-supervised training and fine-tuning","text":"now define pre-processing steps recipe. recipe() defined whole dataset order capture full variability numerical variables, well categories present nominal variables. Note tabnet handles categorical variables, don’t need kind transformation . Normalizing numeric variables good idea though. now normalized dataset ready self-supervised training.","code":"rec <- recipe(Class ~ ., lending_club) %>%   step_normalize(all_numeric()) unlabeled_baked_df <- rec %>% prep %>% bake(new_data=unlabeled)"},{"path":"/articles/selfsupervised_training.html","id":"self-supervised-training-step","dir":"Articles","previous_headings":"","what":"Self-supervised training step","title":"Self-supervised training and fine-tuning","text":"Next, pre-train model autoassociative self-supervised learning task. step gives us tabnet_pretrain object contain representation dataset variables interactions latent space. going train 50 epochs batch size 5000 .e. half dataset small enough fit memory. hyperparameters available, going use default values . minutes can get results: may continue training epoch validation loss seems plateau like model start overfit… can identify plot model epoch providing lowest loss validation-set around epoch 35. reuse from_epoch option training continuation, instead default last epoch. historical model weights available checkpoint-ed epoch , case, checkpoint epoch 40 seems good compromise, use from_epoch=40.","code":"mod <- tabnet_pretrain(rec, unlabeled, epochs = 50, valid_split = 0.2, batch_size = 5000, verbose = TRUE) [Epoch 001] Loss: 4.195504 Valid loss: 1.713710                                                                                     [Epoch 002] Loss: 4.589761 Valid loss: 1.474615                                                                                     [Epoch 003] Loss: 1.812671 Valid loss: 1.410668                                                                                     [Epoch 004] Loss: 1.553153 Valid loss: 1.359872                                                                                     [Epoch 005] Loss: 2.345665 Valid loss: 1.299493                                                                                     [Epoch 006] Loss: 2.719557 Valid loss: 1.258007                                                                                     [Epoch 007] Loss: 1.285357 Valid loss: 1.233827                                                                                     [Epoch 008] Loss: 1.283360 Valid loss: 1.210789                                                                                     [Epoch 009] Loss: 1.452972 Valid loss: 1.194631                                                                                     [Epoch 010] Loss: 1.256993 Valid loss: 1.181094                                                                                     [Epoch 011] Loss: 1.327342 Valid loss: 1.158956                                                                                     [Epoch 012] Loss: 1.258828 Valid loss: 1.145682                                                                                     [Epoch 013] Loss: 1.130475 Valid loss: 1.130623 ... [Epoch 041] Loss: 1.002896 Valid loss: 0.950189                                                                                     [Epoch 042] Loss: 1.142027 Valid loss: 0.944114                                                                                     [Epoch 043] Loss: 0.940986 Valid loss: 0.940836                                                                                     [Epoch 044] Loss: 1.032234 Valid loss: 0.939989                                                                                     [Epoch 045] Loss: 0.947644 Valid loss: 0.937613                                                                                     [Epoch 046] Loss: 1.007923 Valid loss: 0.935926                                                                                     [Epoch 047] Loss: 1.721710 Valid loss: 0.938697                                                                                     [Epoch 048] Loss: 0.984387 Valid loss: 0.941066                                                                                     [Epoch 049] Loss: 1.092131 Valid loss: 0.944751                                                                                     [Epoch 050] Loss: 1.175343 Valid loss: 0.947859 autoplot(mod)"},{"path":"/articles/selfsupervised_training.html","id":"continuing-training-with-supervised-task","dir":"Articles","previous_headings":"","what":"Continuing training with supervised task","title":"Self-supervised training and fine-tuning","text":"Now, reuse pre-processing steps recipe feed directly supervised fitting task top pre-trained model. training continue starting epoch 41 expected ending epoch 90. Now let’s diagnose model training new autoplot(): can see model starts overfit epoch 52 validation-set loss reaches minimum. consider epoch 54 epoch move production, can redo training checkpoint 50 4 epochs : Now, predict() tabnet_explain() functions use model last epoch, epoch 54. Finally, can measure results test set:","code":"model_fit <- tabnet_fit(rec, train , tabnet_model = mod, from_epoch=40, valid_split = 0.2, epochs = 50, verbose=TRUE) [Epoch 041] Loss: 0.260110 Valid loss: 0.250054                                                                                     [Epoch 042] Loss: 0.213628 Valid loss: 0.219784                                                                                     [Epoch 043] Loss: 0.154529 Valid loss: 0.201305                                                                                     [Epoch 044] Loss: 0.187461 Valid loss: 0.202042                                                                                     [Epoch 045] Loss: 0.144463 Valid loss: 0.196665                                                                                     [Epoch 046] Loss: 0.191665 Valid loss: 0.196853                                                                                     [Epoch 047] Loss: 0.186484 Valid loss: 0.190732                                                                                     [Epoch 048] Loss: 0.166680 Valid loss: 0.188041                                                                                     [Epoch 049] Loss: 0.134771 Valid loss: 0.180121                                                                                     [Epoch 050] Loss: 0.127854 Valid loss: 0.168102    ... [Epoch 082] Loss: 0.101062 Valid loss: 0.262795                                                                                     [Epoch 083] Loss: 0.097425 Valid loss: 0.280869                                                                                     [Epoch 084] Loss: 0.113473 Valid loss: 0.296119                                                                                     [Epoch 085] Loss: 0.094036 Valid loss: 0.303786                                                                                     [Epoch 086] Loss: 0.092469 Valid loss: 0.309182                                                                                     [Epoch 087] Loss: 0.117400 Valid loss: 0.316657                                                                                     [Epoch 088] Loss: 0.097304 Valid loss: 0.338525                                                                                     [Epoch 089] Loss: 0.091212 Valid loss: 0.341918                                                                                     [Epoch 090] Loss: 0.087092 Valid loss: 0.361795 autoplot(model_fit) model_fit <- tabnet_fit(rec, train , tabnet_model = model_fit, from_epoch=50, epochs = 4, valid_split = 0.2, verbose=TRUE) test %>%    bind_cols(     predict(model_fit, test, type = \"prob\")   ) %>%    roc_auc(Class, .pred_bad) # A tibble: 1 × 3   .metric .estimator .estimate   <chr>   <chr>          <dbl> 1 roc_auc binary         0.653"},{"path":"/articles/selfsupervised_training.html","id":"comparing-against-a-model-without-pretraining","dir":"Articles","previous_headings":"","what":"Comparing against a model without pretraining","title":"Self-supervised training and fine-tuning","text":"question now “pretrain model ?” can build vanilla tabnet model train dataset comparison : model overfits around epoch 20, restore checkpoint epoch 20 retrain 1 epoch , proceed prediction : can see much lower ROC-AUC compared using pretraining step.","code":"vanilla_model_fit <- tabnet_fit(rec, train , valid_split = 0.2, epochs = 50, verbose=TRUE) [Epoch 001] Loss: 0.725771 Valid loss: 0.491390                                                                                     [Epoch 002] Loss: 0.328243 Valid loss: 0.268964                                                                                     [Epoch 003] Loss: 0.241829 Valid loss: 0.181927                                                                                     [Epoch 004] Loss: 0.205124 Valid loss: 0.166245                                                                                     [Epoch 005] Loss: 0.218578 Valid loss: 0.161050                                                                                     [Epoch 006] Loss: 0.192196 Valid loss: 0.150333                                                                                     [Epoch 007] Loss: 0.235362 Valid loss: 0.148007                                                                                     [Epoch 008] Loss: 0.167538 Valid loss: 0.151358                                                                                     [Epoch 009] Loss: 0.146265 Valid loss: 0.155682                                                                                     [Epoch 010] Loss: 0.168639 Valid loss: 0.154740                                                                                     [Epoch 011] Loss: 0.164683 Valid loss: 0.153092                                                                                     [Epoch 012] Loss: 0.166895 Valid loss: 0.152931                                                                                     [Epoch 013] Loss: 0.153897 Valid loss: 0.155470                                                                                     ... [Epoch 040] Loss: 0.125999 Valid loss: 0.184985                                                                                     [Epoch 041] Loss: 0.116995 Valid loss: 0.173780                                                                                     [Epoch 042] Loss: 0.161852 Valid loss: 0.166246                                                                                     [Epoch 043] Loss: 0.144460 Valid loss: 0.171052                                                                                     [Epoch 044] Loss: 0.146622 Valid loss: 0.170186                                                                                     [Epoch 045] Loss: 0.138121 Valid loss: 0.165647                                                                                     [Epoch 046] Loss: 0.130105 Valid loss: 0.154928                                                                                     [Epoch 047] Loss: 0.123869 Valid loss: 0.158995                                                                                     [Epoch 048] Loss: 0.139626 Valid loss: 0.159415                                                                                     [Epoch 049] Loss: 0.125219 Valid loss: 0.151668                                                                                     [Epoch 050] Loss: 0.129994 Valid loss: 0.154256 autoplot(vanilla_model_fit) vanilla_model_fit <- tabnet_fit(rec, train , tabnet_model= vanilla_model_fit, from_epoch=20, valid_split = 0.2, epochs = 1, verbose=TRUE) test %>%    bind_cols(     predict(vanilla_model_fit, test, type = \"prob\")   ) %>%    roc_auc(Class, .pred_good) # A tibble: 1 x 3   .metric .estimator .estimate   <chr>   <chr>          <dbl> 1 roc_auc binary         0.557"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Daniel Falbel. Author. RStudio. Copyright holder. Christophe Regouby. Maintainer, contributor. Egill Fridgeirsson. Contributor. Philipp Haarmeyer. Contributor. Sven Verweij. Contributor.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Falbel D (2026). tabnet: Fit 'TabNet' Models Classification Regression. R package version 0.8.0.9000, https://mlverse.github.io/tabnet/.","code":"@Manual{,   title = {tabnet: Fit 'TabNet' Models for Classification and Regression},   author = {Daniel Falbel},   year = {2026},   note = {R package version 0.8.0.9000},   url = {https://mlverse.github.io/tabnet/}, }"},{"path":"/index.html","id":"tabnet","dir":"","previous_headings":"","what":"Fit TabNet Models for Classification and Regression","title":"Fit TabNet Models for Classification and Regression","text":"R implementation : TabNet: Attentive Interpretable Tabular Learning (Sercan O. Arik, Tomas Pfister). code repository started R port using torch package dreamquark-ai/tabnet implementation. TabNet now augmented Coherent Hierarchical Multi-label Classification Networks (Eleonora Giunchiglia et Al.) hierarchical outcomes Optimizing ROC Curves Sort-Based Surrogate Loss Binary Classification Changepoint Detection (J Hillman, TD Hocking) imbalanced binary classification.","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Fit TabNet Models for Classification and Regression","text":"Install {tabnet} CRAN : development version can installed GitHub :","code":"install.packages('tabnet') # install.packages(\"pak\") pak::pak(\"mlverse/tabnet\")"},{"path":"/index.html","id":"basic-binary-classification-example","dir":"","previous_headings":"","what":"Basic Binary Classification Example","title":"Fit TabNet Models for Classification and Regression","text":"show binary classification example attrition dataset, using recipe dataset input specification.  plots gives immediate insight model -fitting, , available model checkpoints available -fitting Keep mind regression well multi-class classification also available, can specify dataset data.frame formula well. find package vignettes.","code":"library(tabnet) suppressPackageStartupMessages(library(recipes)) library(yardstick) library(ggplot2) set.seed(1)  data(\"attrition\", package = \"modeldata\") test_idx <- sample.int(nrow(attrition), size = 0.2 * nrow(attrition))  train <- attrition[-test_idx,] test <- attrition[test_idx,]  rec <- recipe(Attrition ~ ., data = train) %>%    step_normalize(all_numeric(), -all_outcomes())  fit <- tabnet_fit(rec, train, epochs = 30, valid_split=0.1, learn_rate = 5e-3) autoplot(fit)"},{"path":"/index.html","id":"model-performance-results","dir":"","previous_headings":"","what":"Model performance results","title":"Fit TabNet Models for Classification and Regression","text":"standard method predict() used, can rely usual metric functions model performance results. use {yardstick} :","code":"metrics <- metric_set(accuracy, precision, recall) cbind(test, predict(fit, test)) %>%    metrics(Attrition, estimate = .pred_class) #> # A tibble: 3 × 3 #>   .metric   .estimator .estimate #>   <chr>     <chr>          <dbl> #> 1 accuracy  binary         0.840 #> 2 precision binary         0.840 #> 3 recall    binary         1    cbind(test, predict(fit, test, type = \"prob\")) %>%    roc_auc(Attrition, .pred_No) #> # A tibble: 1 × 3 #>   .metric .estimator .estimate #>   <chr>   <chr>          <dbl> #> 1 roc_auc binary         0.466"},{"path":"/index.html","id":"explain-model-on-test-set-with-attention-map","dir":"","previous_headings":"","what":"Explain model on test-set with attention map","title":"Fit TabNet Models for Classification and Regression","text":"TabNet intrinsic explainability feature visualization attention map, either aggregated:  layer type = \"steps\" option:","code":"explain <- tabnet_explain(fit, test) autoplot(explain) autoplot(explain, type = \"steps\")"},{"path":"/index.html","id":"self-supervised-pretraining","dir":"","previous_headings":"","what":"Self-supervised pretraining","title":"Fit TabNet Models for Classification and Regression","text":"cases consistent part dataset outcome, TabNet offers self-supervised training step allowing model capture predictors intrinsic features predictors interactions, upfront supervised task.  example toy example train dataset actually contain outcomes. vignette vignette(\"selfsupervised_training\") gives complete correct workflow step--step.","code":"pretrain <- tabnet_pretrain(rec, train, epochs = 50, valid_split=0.1, learn_rate = 1e-2) autoplot(pretrain)"},{"path":"/index.html","id":"tidymodels-integration","dir":"","previous_headings":"","what":"{tidymodels} integration","title":"Fit TabNet Models for Classification and Regression","text":"integration within tidymodels workflows offers unlimited opportunity compare {tabnet} models challengers. Don’t miss vignette(\"tidymodels-interface\") .","code":""},{"path":"/index.html","id":"missing-data-in-predictors","dir":"","previous_headings":"","what":"Missing data in predictors","title":"Fit TabNet Models for Classification and Regression","text":"{tabnet} leverage masking mechanism deal missing data, don’t remove entries dataset missing values predictors variables. See vignette(\"Missing_data_predictors\")","code":""},{"path":"/index.html","id":"imbalanced-binary-classification","dir":"","previous_headings":"","what":"Imbalanced binary classification","title":"Fit TabNet Models for Classification and Regression","text":"{tabnet} includes Area Min(FPR,FNR)Min(FPR,FNR) (AUM) loss function nn_aum_loss() dedicated imbalanced binary classification tasks. Try vignette(\"aum_loss\")","code":""},{"path":"/index.html","id":"comparison-with-other-implementations","dir":"","previous_headings":"","what":"Comparison with other implementations","title":"Fit TabNet Models for Classification and Regression","text":"Alternative TabNet implementation features","code":""},{"path":"/reference/autoplot.tabnet_explain.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot tabnet_explain mask importance heatmap — autoplot.tabnet_explain","title":"Plot tabnet_explain mask importance heatmap — autoplot.tabnet_explain","text":"Plot tabnet_explain mask importance heatmap","code":""},{"path":"/reference/autoplot.tabnet_explain.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot tabnet_explain mask importance heatmap — autoplot.tabnet_explain","text":"","code":"autoplot.tabnet_explain(   object,   type = c(\"mask_agg\", \"steps\"),   quantile = 1,   ... )"},{"path":"/reference/autoplot.tabnet_explain.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot tabnet_explain mask importance heatmap — autoplot.tabnet_explain","text":"object tabnet_explain object result tabnet_explain(). type character value. Either \"mask_agg\" default, single heatmap aggregated mask importance per predictor along dataset, \"steps\" one heatmap mask step. quantile numerical value 0 1. Provides quantile clipping mask values ... used.","code":""},{"path":"/reference/autoplot.tabnet_explain.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot tabnet_explain mask importance heatmap — autoplot.tabnet_explain","text":"ggplot object.","code":""},{"path":"/reference/autoplot.tabnet_explain.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot tabnet_explain mask importance heatmap — autoplot.tabnet_explain","text":"Plot tabnet_explain object mask importance per variable along predicted dataset. type=\"mask_agg\" output single heatmap mask aggregated values, type=\"steps\" provides plot faceted along n_steps mask present model. quantile=.995 may used strong outlier clipping, order better highlight low values. quantile=1, default, clip values.","code":""},{"path":"/reference/autoplot.tabnet_explain.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot tabnet_explain mask importance heatmap — autoplot.tabnet_explain","text":"","code":"if (FALSE) { # \\dontrun{ library(ggplot2) data(\"attrition\", package = \"modeldata\")  ## Single-outcome binary classification of `Attrition` in `attrition` dataset attrition_fit <- tabnet_fit(Attrition ~. , data=attrition, epoch=11) attrition_explain <- tabnet_explain(attrition_fit, attrition) # Plot the model aggregated mask interpretation heatmap autoplot(attrition_explain)  ## Multi-outcome regression on `Sale_Price` and `Pool_Area` in `ames` dataset, data(\"ames\", package = \"modeldata\") x <- ames[,-which(names(ames) %in% c(\"Sale_Price\", \"Pool_Area\"))] y <- ames[, c(\"Sale_Price\", \"Pool_Area\")] ames_fit <- tabnet_fit(x, y, epochs = 1, verbose=TRUE) ames_explain <- tabnet_explain(ames_fit, x) autoplot(ames_explain, quantile = 0.99) } # }"},{"path":"/reference/autoplot.tabnet_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot tabnet_fit model loss along epochs — autoplot.tabnet_fit","title":"Plot tabnet_fit model loss along epochs — autoplot.tabnet_fit","text":"Plot tabnet_fit model loss along epochs","code":""},{"path":"/reference/autoplot.tabnet_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot tabnet_fit model loss along epochs — autoplot.tabnet_fit","text":"","code":"autoplot.tabnet_fit(object, ...)  autoplot.tabnet_pretrain(object, ...)"},{"path":"/reference/autoplot.tabnet_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot tabnet_fit model loss along epochs — autoplot.tabnet_fit","text":"object tabnet_fit tabnet_pretrain object result tabnet_fit() tabnet_pretrain(). ... used.","code":""},{"path":"/reference/autoplot.tabnet_fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot tabnet_fit model loss along epochs — autoplot.tabnet_fit","text":"ggplot object.","code":""},{"path":"/reference/autoplot.tabnet_fit.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot tabnet_fit model loss along epochs — autoplot.tabnet_fit","text":"Plot training loss along epochs, validation loss along epochs . dot added epochs model snapshot available, helping choice from_epoch value later model training resume.","code":""},{"path":"/reference/autoplot.tabnet_fit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot tabnet_fit model loss along epochs — autoplot.tabnet_fit","text":"","code":"if (FALSE) { # \\dontrun{ library(ggplot2) data(\"attrition\", package = \"modeldata\") attrition_fit <- tabnet_fit(Attrition ~. , data=attrition, valid_split=0.2, epoch=11)  # Plot the model loss over epochs autoplot(attrition_fit) } # }"},{"path":"/reference/check_compliant_node.html","id":null,"dir":"Reference","previous_headings":"","what":"Check that Node object names are compliant — check_compliant_node","title":"Check that Node object names are compliant — check_compliant_node","text":"Check Node object names compliant","code":""},{"path":"/reference/check_compliant_node.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check that Node object names are compliant — check_compliant_node","text":"","code":"check_compliant_node(node)"},{"path":"/reference/check_compliant_node.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check that Node object names are compliant — check_compliant_node","text":"node Node object, dataframe ready parsed data.tree::.Node()","code":""},{"path":"/reference/check_compliant_node.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check that Node object names are compliant — check_compliant_node","text":"node compliant, else Error column names fix","code":""},{"path":"/reference/check_compliant_node.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check that Node object names are compliant — check_compliant_node","text":"","code":"library(dplyr) #>  #> Attaching package: ‘dplyr’ #> The following objects are masked from ‘package:stats’: #>  #>     filter, lag #> The following objects are masked from ‘package:base’: #>  #>     intersect, setdiff, setequal, union library(data.tree) data(starwars) starwars_tree <- starwars %>%   mutate(pathString = paste(\"tree\", species, homeworld, `name`, sep = \"/\"))  # pre as.Node() check try(check_compliant_node(starwars_tree)) #> Error in check_compliant_node(starwars_tree) :  #>   The attributes or colnames in the provided hierarchical object use the #> following reserved names: name and height.  Please change those names as they #> will lead to unexpected tabnet behavior.  # post as.Node() check check_compliant_node(as.Node(starwars_tree))"},{"path":"/reference/entmax15.html","id":null,"dir":"Reference","previous_headings":"","what":"Alpha-entmax — entmax","title":"Alpha-entmax — entmax","text":"alpha = 1.5 normalizing sparse transform (la softmax).","code":""},{"path":"/reference/entmax15.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Alpha-entmax — entmax","text":"","code":"entmax(dim = -1)  entmax15(dim = -1L, k = NULL)"},{"path":"/reference/entmax15.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Alpha-entmax — entmax","text":"dim dimension along apply 1.5-entmax. k number largest elements partial-sort input . optimal performance, slightly bigger expected number non-zeros solution. solution k-sparse, function recursively called 2*k schedule. NULL, full sorting performed beginning.","code":""},{"path":"/reference/entmax15.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Alpha-entmax — entmax","text":"projection result P  shape input, \\(\\sum_{dim} P = 1 \\forall dim\\) elementwise.","code":""},{"path":"/reference/entmax15.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Alpha-entmax — entmax","text":"Solves optimization problem: \\(\\max_p <input, P> - H_{1.5}(P) \\text{ s.t. } P \\geq 0, \\sum(P) == 1\\) \\(H_{1.5}(P)\\) Tsallis alpha-entropy \\(\\alpha=1.5\\).","code":""},{"path":"/reference/entmax15.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Alpha-entmax — entmax","text":"","code":"if (FALSE) { # \\dontrun{ input <- torch::torch_randn(10,5, requires_grad = TRUE) # create a top3 alpha=1.5 entmax on last input dimension nn_entmax <- entmax15(dim=-1L, k = 3) result <- nn_entmax(input) } # }"},{"path":"/reference/get_tau.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimal threshold (tau) computation for 1.5-entmax — get_tau","title":"Optimal threshold (tau) computation for 1.5-entmax — get_tau","text":"Optimal threshold (tau) computation 1.5-entmax","code":""},{"path":"/reference/get_tau.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Optimal threshold (tau) computation for 1.5-entmax — get_tau","text":"","code":"get_tau(input, dim = -1L, k = NULL)"},{"path":"/reference/get_tau.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Optimal threshold (tau) computation for 1.5-entmax — get_tau","text":"input input tensor compute thresholds . dim dimension along apply 1.5-entmax. Default -1. k number largest elements partial-sort . optimal performance, slightly bigger expected number non-zeros solution. solution k-sparse, function recursively called 2*k schedule. NULL, full sorting performed beginning. Default NULL.","code":""},{"path":"/reference/get_tau.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Optimal threshold (tau) computation for 1.5-entmax — get_tau","text":"threshold value vector, dim dimension intact.","code":""},{"path":"/reference/min_grid.tabnet.html","id":null,"dir":"Reference","previous_headings":"","what":"Determine the minimum set of model fits — min_grid.tabnet","title":"Determine the minimum set of model fits — min_grid.tabnet","text":"min_grid() determines exactly models fit order evaluate entire set tuning parameter combinations. internal use API may change near future.","code":""},{"path":"/reference/min_grid.tabnet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Determine the minimum set of model fits — min_grid.tabnet","text":"","code":"# S3 method for class 'tabnet' min_grid(x, grid, ...)"},{"path":"/reference/min_grid.tabnet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Determine the minimum set of model fits — min_grid.tabnet","text":"x model specification. grid tibble tuning parameter combinations. ... currently used.","code":""},{"path":"/reference/min_grid.tabnet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Determine the minimum set of model fits — min_grid.tabnet","text":"tibble minimum tuning parameters fit additional list column parameter combinations used prediction.","code":""},{"path":"/reference/min_grid.tabnet.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Determine the minimum set of model fits — min_grid.tabnet","text":"fit_max_value() can used packages implement min_grid() method.","code":""},{"path":"/reference/min_grid.tabnet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Determine the minimum set of model fits — min_grid.tabnet","text":"","code":"library(dials) #> Loading required package: scales #>  #> Attaching package: ‘dials’ #> The following objects are masked from ‘package:tabnet’: #>  #>     momentum, penalty library(tune) library(parsnip)  tabnet_spec <- tabnet(decision_width = tune(), attention_width = tune()) %>%   set_mode(\"regression\") %>%   set_engine(\"torch\")  tabnet_grid <-   tabnet_spec %>%   extract_parameter_set_dials() %>%   grid_regular(levels = 3)  min_grid(tabnet_spec, tabnet_grid) #> # A tibble: 9 × 3 #>   decision_width attention_width .submodels #>            <int>           <int> <list>     #> 1              8               8 <list [0]> #> 2             36               8 <list [0]> #> 3             64               8 <list [0]> #> 4              8              36 <list [0]> #> 5             36              36 <list [0]> #> 6             64              36 <list [0]> #> 7              8              64 <list [0]> #> 8             36              64 <list [0]> #> 9             64              64 <list [0]>"},{"path":"/reference/nn_aum_loss.html","id":null,"dir":"Reference","previous_headings":"","what":"AUM loss — nn_aum_loss","title":"AUM loss — nn_aum_loss","text":"Creates criterion measures Area \\(Min(FPR, FNR)\\) (AUM) element input \\(pred_tensor\\) target \\(label_tensor\\).","code":""},{"path":"/reference/nn_aum_loss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"AUM loss — nn_aum_loss","text":"","code":"nn_aum_loss()"},{"path":"/reference/nn_aum_loss.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"AUM loss — nn_aum_loss","text":"used measuring error binary reconstruction within highly unbalanced dataset, goal optimizing ROC curve. Note targets \\(label_tensor\\) factor level binary outcome, .e. values 1L 2L.","code":""},{"path":"/reference/nn_aum_loss.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"AUM loss — nn_aum_loss","text":"","code":"loss <- nn_aum_loss() input <- torch::torch_randn(4, 6, requires_grad = TRUE) target <- input > 1.5 output <- loss(input, target) output$backward()"},{"path":"/reference/nn_prune_head.html","id":null,"dir":"Reference","previous_headings":"","what":"Prune top layer(s) of a tabnet network — nn_prune_head.tabnet_fit","title":"Prune top layer(s) of a tabnet network — nn_prune_head.tabnet_fit","text":"Prune head_size last layers tabnet network order use pruned module sequential embedding module.","code":""},{"path":"/reference/nn_prune_head.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prune top layer(s) of a tabnet network — nn_prune_head.tabnet_fit","text":"","code":"# S3 method for class 'tabnet_fit' nn_prune_head(x, head_size)  # S3 method for class 'tabnet_pretrain' nn_prune_head(x, head_size)"},{"path":"/reference/nn_prune_head.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prune top layer(s) of a tabnet network — nn_prune_head.tabnet_fit","text":"x nn_network prune head_size number nn_layers prune, less 2","code":""},{"path":"/reference/nn_prune_head.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prune top layer(s) of a tabnet network — nn_prune_head.tabnet_fit","text":"tabnet network top nn_layer removed","code":""},{"path":"/reference/nn_prune_head.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prune top layer(s) of a tabnet network — nn_prune_head.tabnet_fit","text":"","code":"data(\"ames\", package = \"modeldata\") x <- ames[,-which(names(ames) == \"Sale_Price\")] y <- ames$Sale_Price # pretrain a tabnet model on ames dataset ames_pretrain <- tabnet_pretrain(x, y, epoch = 2, checkpoint_epochs = 1) # prune classification head to get an embedding model pruned_pretrain <- torch::nn_prune_head(ames_pretrain, 1)"},{"path":"/reference/node_to_df.html","id":null,"dir":"Reference","previous_headings":"","what":"Turn a Node object into predictor and outcome. — node_to_df","title":"Turn a Node object into predictor and outcome. — node_to_df","text":"Turn Node object predictor outcome.","code":""},{"path":"/reference/node_to_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Turn a Node object into predictor and outcome. — node_to_df","text":"","code":"node_to_df(x, drop_last_level = TRUE)"},{"path":"/reference/node_to_df.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Turn a Node object into predictor and outcome. — node_to_df","text":"x Node object drop_last_level TRUE unused","code":""},{"path":"/reference/node_to_df.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Turn a Node object into predictor and outcome. — node_to_df","text":"named list x y, respectively predictor data-frame outcomes data-frame, expected inputs hardhat::mold() function.","code":""},{"path":"/reference/node_to_df.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Turn a Node object into predictor and outcome. — node_to_df","text":"","code":"library(dplyr) library(data.tree) data(starwars) starwars_tree <- starwars %>%   mutate(pathString = paste(\"tree\", species, homeworld, `name`, sep = \"/\")) %>%   as.Node() node_to_df(starwars_tree)$x %>% head() #>   birth_year eye_color #> 1       19.0      blue #> 2       41.9    yellow #> 3       52.0      blue #> 4       47.0      blue #> 5       24.0     brown #> 6       41.9      blue #>                                                                                             films #> 1 A New Hope, The Empire Strikes Back, Return of the Jedi, Revenge of the Sith, The Force Awakens #> 2                    A New Hope, The Empire Strikes Back, Return of the Jedi, Revenge of the Sith #> 3                                           A New Hope, Attack of the Clones, Revenge of the Sith #> 4                                           A New Hope, Attack of the Clones, Revenge of the Sith #> 5                                                                                      A New Hope #> 6                                   The Phantom Menace, Attack of the Clones, Revenge of the Sith #>      gender  hair_color homeworld mass    sex skin_color species #> 1 masculine       blond  Tatooine   77   male       fair   Human #> 2 masculine        none  Tatooine  136   male      white   Human #> 3 masculine brown, grey  Tatooine  120   male      light   Human #> 4  feminine       brown  Tatooine   75 female      light   Human #> 5 masculine       black  Tatooine   84   male      light   Human #> 6 masculine       blond  Tatooine   84   male       fair   Human #>                                                   starships #> 1                                  X-wing, Imperial shuttle #> 2                                           TIE Advanced x1 #> 3                                                      <NA> #> 4                                                      <NA> #> 5                                                    X-wing #> 6 Naboo fighter, Trade Federation cruiser, Jedi Interceptor #>                               vehicles #> 1   Snowspeeder, Imperial Speeder Bike #> 2                                 <NA> #> 3                                 <NA> #> 4                                 <NA> #> 5                                 <NA> #> 6 Zephyr-G swoop bike, XJ-6 airspeeder node_to_df(starwars_tree)$y %>% head() #>   level_2  level_3 #> 1   Human Tatooine #> 2   Human Tatooine #> 3   Human Tatooine #> 4   Human Tatooine #> 5   Human Tatooine #> 6   Human Tatooine"},{"path":"/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::%>% details.","code":""},{"path":"/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"/reference/pipe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pipe operator — %>%","text":"Returns rhs(lhs).","code":""},{"path":"/reference/sparsemax.html","id":null,"dir":"Reference","previous_headings":"","what":"Sparsemax — sparsemax","title":"Sparsemax — sparsemax","text":"Normalizing sparse transform (la softmax).","code":""},{"path":"/reference/sparsemax.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sparsemax — sparsemax","text":"","code":"sparsemax(dim = -1L)  sparsemax15(dim = -1L, k = NULL)"},{"path":"/reference/sparsemax.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sparsemax — sparsemax","text":"dim dimension along apply sparsemax. k number largest elements partial-sort input . optimal performance, k slightly bigger expected number non-zeros solution. solution k-sparse, function recursively called 2*k schedule. NULL, full sorting performed beginning.","code":""},{"path":"/reference/sparsemax.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sparsemax — sparsemax","text":"projection result, \\(\\sum_{dim} P = 1 \\forall dim\\) elementwise.","code":""},{"path":"/reference/sparsemax.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sparsemax — sparsemax","text":"Solves projection: \\(\\min_P ||input - P||_2 \\text{ s.t. } P \\geq0, \\sum(P) ==1\\)","code":""},{"path":"/reference/sparsemax.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sparsemax — sparsemax","text":"","code":"if (FALSE) { # \\dontrun{ input <- torch::torch_randn(10, 5, requires_grad = TRUE) # create a top3 alpha=1.5 sparsemax on last input dimension nn_sparsemax <- sparsemax15(dim=1, k=3) result <- nn_sparsemax(input) print(result) } # }"},{"path":"/reference/tabnet.html","id":null,"dir":"Reference","previous_headings":"","what":"Parsnip compatible tabnet model — tabnet","title":"Parsnip compatible tabnet model — tabnet","text":"Parsnip compatible tabnet model","code":""},{"path":"/reference/tabnet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parsnip compatible tabnet model — tabnet","text":"","code":"tabnet(   mode = \"unknown\",   cat_emb_dim = NULL,   decision_width = NULL,   attention_width = NULL,   num_steps = NULL,   mask_type = NULL,   mask_topk = NULL,   num_independent = NULL,   num_shared = NULL,   num_independent_decoder = NULL,   num_shared_decoder = NULL,   penalty = NULL,   feature_reusage = NULL,   momentum = NULL,   epochs = NULL,   batch_size = NULL,   virtual_batch_size = NULL,   learn_rate = NULL,   optimizer = NULL,   loss = NULL,   clip_value = NULL,   drop_last = NULL,   lr_scheduler = NULL,   rate_decay = NULL,   rate_step_size = NULL,   checkpoint_epochs = NULL,   verbose = NULL,   importance_sample_size = NULL,   early_stopping_monitor = NULL,   early_stopping_tolerance = NULL,   early_stopping_patience = NULL,   skip_importance = NULL,   tabnet_model = NULL,   from_epoch = NULL )"},{"path":"/reference/tabnet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parsnip compatible tabnet model — tabnet","text":"mode single character string type model. Possible values model \"unknown\", \"regression\", \"classification\". cat_emb_dim Size embedding categorical features. int, categorical features embedding size, list int, every corresponding feature specific embedding size. decision_width (int) Width decision prediction layer. Bigger values gives capacity model risk overfitting. Values typically range 8 64. attention_width (int) Width attention embedding mask. According paper n_d = n_a usually good choice. (default=8) num_steps (int) Number steps architecture (usually 3 10) mask_type (character) Final layer feature selector attentive_transformer block, either \"sparsemax\", \"entmax\" \"entmax15\".Defaults \"sparsemax\". mask_topk (int) mask sparsity top-k sparsemax15 entmax15. See entmax15() detail. num_independent Number independent Gated Linear Units layers step encoder. Usual values range 1 5. num_shared Number shared Gated Linear Units step encoder. Usual values step decoder. range 1 5 num_independent_decoder pretraining, number independent Gated Linear Units layers Usual values range 1 5. num_shared_decoder pretraining, number shared Gated Linear Units step decoder. Usual values range 1 5. penalty extra sparsity loss coefficient proposed original paper. bigger coefficient , sparser model terms feature selection. Depending difficulty problem, reducing value help (default 1e-3). feature_reusage (num) coefficient feature reusage masks. value close 1 make mask selection least correlated layers. Values range 1 2. momentum Momentum batch normalization, typically ranges 0.01 0.4 (default=0.02) epochs (int) Number training epochs. batch_size (int) Number examples per batch, large batch sizes recommended. (default: 1024^2) virtual_batch_size (int) Size mini batches used \"Ghost Batch Normalization\" (default=256^2) learn_rate initial learning rate optimizer. optimizer optimization method. currently \"adam\" supported, can also pass torch optimizer function. loss (character function) Loss function training (default mse regression cross entropy classification) clip_value num given clip gradient clip_value. Pass NULL clip. drop_last (logical) Whether drop last batch complete training lr_scheduler NULL, learning rate decay used. \"step\" decays learning rate lr_decay every step_size epochs. \"reduce_on_plateau\" decays learning rate lr_decay improvement step_size epochs. can also torch::lr_scheduler function takes optimizer parameter. step method called per epoch. rate_decay multiplies initial learning rate rate_decay every rate_step_size epochs. Unused lr_scheduler torch::lr_scheduler NULL. rate_step_size learning rate scheduler step size. Unused lr_scheduler torch::lr_scheduler NULL. checkpoint_epochs checkpoint model weights architecture every checkpoint_epochs. (default 10). may cause large memory usage. Use 0 disable checkpoints. verbose (logical) Whether print progress loss values training. importance_sample_size sample dataset compute importance metrics. dataset larger 1e5 obs use sample size 1e5 display warning. early_stopping_monitor Metric monitor early_stopping. One \"valid_loss\", \"train_loss\" \"auto\" (defaults \"auto\"). early_stopping_tolerance Minimum relative improvement reset patience counter. 0.01 1% tolerance (default 0) early_stopping_patience Number epochs without improving stopping training. (default=5) skip_importance feature importance calculation skipped (default: FALSE) tabnet_model previously fitted tabnet_model object continue fitting . NULL (default) brand new model initialized. from_epoch tabnet_model provided, restore network weights specific epoch. Default last available checkpoint restored model, last epoch -memory model.","code":""},{"path":"/reference/tabnet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parsnip compatible tabnet model — tabnet","text":"TabNet parsnip instance. can used fit tabnet models using parsnip machinery.","code":""},{"path":"/reference/tabnet.html","id":"threading","dir":"Reference","previous_headings":"","what":"Threading","title":"Parsnip compatible tabnet model — tabnet","text":"TabNet uses torch backend computation torch uses available threads default. can control number threads used torch :","code":"torch::torch_set_num_threads(1) torch::torch_set_num_interop_threads(1)"},{"path":[]},{"path":"/reference/tabnet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Parsnip compatible tabnet model — tabnet","text":"","code":"library(parsnip) data(\"ames\", package = \"modeldata\") model <- tabnet() %>%   set_mode(\"regression\") %>%   set_engine(\"torch\") model %>%   fit(Sale_Price ~ ., data = ames) #> parsnip model object #>  #> An `nn_module` containing 10,742 parameters. #>  #> ── Modules ───────────────────────────────────────────────────────────────────── #> • embedder: <embedding_generator> #283 parameters #> • embedder_na: <na_embedding_generator> #0 parameters #> • tabnet: <tabnet_no_embedding> #10,458 parameters #>  #> ── Parameters ────────────────────────────────────────────────────────────────── #> • .check: Float [1:1]"},{"path":"/reference/tabnet_config.html","id":null,"dir":"Reference","previous_headings":"","what":"Configuration for TabNet models — tabnet_config","title":"Configuration for TabNet models — tabnet_config","text":"Configuration TabNet models","code":""},{"path":"/reference/tabnet_config.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Configuration for TabNet models — tabnet_config","text":"","code":"tabnet_config(   batch_size = 1024^2,   penalty = 0.001,   clip_value = NULL,   loss = \"auto\",   epochs = 5,   drop_last = FALSE,   decision_width = NULL,   attention_width = NULL,   num_steps = 3,   feature_reusage = 1.3,   mask_type = \"sparsemax\",   mask_topk = NULL,   virtual_batch_size = 256^2,   valid_split = 0,   learn_rate = 0.02,   optimizer = \"adam\",   lr_scheduler = NULL,   lr_decay = 0.1,   step_size = 30,   checkpoint_epochs = 10,   cat_emb_dim = 1,   num_independent = 2,   num_shared = 2,   num_independent_decoder = 1,   num_shared_decoder = 1,   momentum = 0.02,   pretraining_ratio = 0.5,   verbose = FALSE,   device = \"auto\",   importance_sample_size = NULL,   early_stopping_monitor = \"auto\",   early_stopping_tolerance = 0,   early_stopping_patience = 0L,   num_workers = 0L,   skip_importance = FALSE )"},{"path":"/reference/tabnet_config.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Configuration for TabNet models — tabnet_config","text":"batch_size (int) Number examples per batch, large batch sizes recommended. (default: 1024^2) penalty extra sparsity loss coefficient proposed original paper. bigger coefficient , sparser model terms feature selection. Depending difficulty problem, reducing value help (default 1e-3). clip_value num given clip gradient clip_value. Pass NULL clip. loss (character function) Loss function training (default mse regression cross entropy classification) epochs (int) Number training epochs. drop_last (logical) Whether drop last batch complete training decision_width (int) Width decision prediction layer. Bigger values gives capacity model risk overfitting. Values typically range 8 64. attention_width (int) Width attention embedding mask. According paper n_d = n_a usually good choice. (default=8) num_steps (int) Number steps architecture (usually 3 10) feature_reusage (num) coefficient feature reusage masks. value close 1 make mask selection least correlated layers. Values range 1 2. mask_type (character) Final layer feature selector attentive_transformer block, either \"sparsemax\", \"entmax\" \"entmax15\".Defaults \"sparsemax\". mask_topk (int) mask sparsity top-k sparsemax15 entmax15. See entmax15() detail. virtual_batch_size (int) Size mini batches used \"Ghost Batch Normalization\" (default=256^2) valid_split [0, 1). fraction dataset used validation. (default = 0 means split) learn_rate initial learning rate optimizer. optimizer optimization method. currently \"adam\" supported, can also pass torch optimizer function. lr_scheduler NULL, learning rate decay used. \"step\" decays learning rate lr_decay every step_size epochs. \"reduce_on_plateau\" decays learning rate lr_decay improvement step_size epochs. can also torch::lr_scheduler function takes optimizer parameter. step method called per epoch. lr_decay multiplies initial learning rate lr_decay every step_size epochs. Unused lr_scheduler torch::lr_scheduler NULL. step_size learning rate scheduler step size. Unused lr_scheduler torch::lr_scheduler NULL. checkpoint_epochs checkpoint model weights architecture every checkpoint_epochs. (default 10). may cause large memory usage. Use 0 disable checkpoints. cat_emb_dim Size embedding categorical features. int, categorical features embedding size, list int, every corresponding feature specific embedding size. num_independent Number independent Gated Linear Units layers step encoder. Usual values range 1 5. num_shared Number shared Gated Linear Units step encoder. Usual values step decoder. range 1 5 num_independent_decoder pretraining, number independent Gated Linear Units layers Usual values range 1 5. num_shared_decoder pretraining, number shared Gated Linear Units step decoder. Usual values range 1 5. momentum Momentum batch normalization, typically ranges 0.01 0.4 (default=0.02) pretraining_ratio Ratio features mask reconstruction pretraining.  Ranges 0 1 (default=0.5) verbose (logical) Whether print progress loss values training. device device use training. \"cpu\" \"cuda\". default (\"auto\") uses  \"cuda\" available, otherwise uses \"cpu\". importance_sample_size sample dataset compute importance metrics. dataset larger 1e5 obs use sample size 1e5 display warning. early_stopping_monitor Metric monitor early_stopping. One \"valid_loss\", \"train_loss\" \"auto\" (defaults \"auto\"). early_stopping_tolerance Minimum relative improvement reset patience counter. 0.01 1% tolerance (default 0) early_stopping_patience Number epochs without improving stopping training. (default=5) num_workers (int, optional): many subprocesses use data loading. 0 means data loaded main process. (default: 0) skip_importance feature importance calculation skipped (default: FALSE)","code":""},{"path":"/reference/tabnet_config.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Configuration for TabNet models — tabnet_config","text":"named list hyperparameters TabNet implementation.","code":""},{"path":"/reference/tabnet_config.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Configuration for TabNet models — tabnet_config","text":"","code":"data(\"ames\", package = \"modeldata\")  # change the model config for an faster ignite optimizer config <- tabnet_config(optimizer = torch::optim_ignite_adamw)  ## Single-outcome regression using formula specification fit <- tabnet_fit(Sale_Price ~ ., data = ames, epochs = 1, config = config)"},{"path":"/reference/tabnet_explain.html","id":null,"dir":"Reference","previous_headings":"","what":"Interpretation metrics from a TabNet model — tabnet_explain","title":"Interpretation metrics from a TabNet model — tabnet_explain","text":"Interpretation metrics TabNet model","code":""},{"path":"/reference/tabnet_explain.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Interpretation metrics from a TabNet model — tabnet_explain","text":"","code":"tabnet_explain(object, new_data)  # Default S3 method tabnet_explain(object, new_data)  # S3 method for class 'tabnet_fit' tabnet_explain(object, new_data)  # S3 method for class 'tabnet_pretrain' tabnet_explain(object, new_data)  # S3 method for class 'model_fit' tabnet_explain(object, new_data)"},{"path":"/reference/tabnet_explain.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Interpretation metrics from a TabNet model — tabnet_explain","text":"object TabNet fit object new_data data.frame obtain interpretation metrics.","code":""},{"path":"/reference/tabnet_explain.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Interpretation metrics from a TabNet model — tabnet_explain","text":"Returns list M_explain: aggregated feature importance masks detailed TabNet's paper. masks list containing masks step.","code":""},{"path":"/reference/tabnet_explain.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Interpretation metrics from a TabNet model — tabnet_explain","text":"","code":"set.seed(2021)  n <- 256 x <- data.frame(   x = rnorm(n),   y = rnorm(n),   z = rnorm(n) )  y <- x$x  fit <- tabnet_fit(x, y, epochs = 10,                   num_steps = 1,                   batch_size = 512,                   attention_width = 1,                   num_shared = 1,                   num_independent = 1)    ex <- tabnet_explain(fit, x)"},{"path":"/reference/tabnet_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Tabnet model — tabnet_fit","title":"Tabnet model — tabnet_fit","text":"Fits TabNet: Attentive Interpretable Tabular Learning model","code":""},{"path":"/reference/tabnet_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tabnet model — tabnet_fit","text":"","code":"tabnet_fit(x, ...)  # Default S3 method tabnet_fit(x, ...)  # S3 method for class 'data.frame' tabnet_fit(   x,   y,   tabnet_model = NULL,   config = tabnet_config(),   ...,   from_epoch = NULL,   weights = NULL )  # S3 method for class 'formula' tabnet_fit(   formula,   data,   tabnet_model = NULL,   config = tabnet_config(),   ...,   from_epoch = NULL,   weights = NULL )  # S3 method for class 'recipe' tabnet_fit(   x,   data,   tabnet_model = NULL,   config = tabnet_config(),   ...,   from_epoch = NULL,   weights = NULL )  # S3 method for class 'Node' tabnet_fit(   x,   tabnet_model = NULL,   config = tabnet_config(),   ...,   from_epoch = NULL )"},{"path":"/reference/tabnet_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tabnet model — tabnet_fit","text":"x Depending context: data frame predictors. matrix predictors. recipe specifying set preprocessing steps created recipes::recipe(). Node tree used hierarchical outcome, attributes used predictors. predictor data standardized (e.g. centered scaled). model treats categorical predictors internally thus, need make treatment. model treats missing values internally thus, need make treatment. ... Model hyperparameters. hyperparameters set update set config argument. See tabnet_config() list possible hyperparameters. y x data frame matrix, y outcome specified : data frame 1 many numeric column (regression) 1 many categorical columns (classification) . matrix 1 column. vector, either numeric categorical. tabnet_model previously fitted tabnet_model object continue fitting . NULL (default) brand new model initialized. config set hyperparameters created using tabnet_config function. argument supplied, use default values tabnet_config(). from_epoch tabnet_model provided, restore network weights specific epoch. Default last available checkpoint restored model, last epoch -memory model. weights Unused. Placeholder hardhat::importance_weight() variables. formula formula specifying outcome terms left-hand side, predictor terms right-hand side. data recipe formula used, data specified : data frame containing predictors outcome.","code":""},{"path":"/reference/tabnet_fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tabnet model — tabnet_fit","text":"TabNet model object. can used serialization, predictions, fitting.","code":""},{"path":"/reference/tabnet_fit.html","id":"fitting-a-pre-trained-model","dir":"Reference","previous_headings":"","what":"Fitting a pre-trained model","title":"Tabnet model — tabnet_fit","text":"providing parent tabnet_model parameter, model fitting resumes model weights following epoch: last fitted epoch model already torch context Last model checkpoint epoch model loaded file epoch related checkpoint matching preceding from_epoch value provided model fitting metrics append top parent metrics returned TabNet model.","code":""},{"path":"/reference/tabnet_fit.html","id":"multi-outcome","dir":"Reference","previous_headings":"","what":"Multi-outcome","title":"Tabnet model — tabnet_fit","text":"TabNet allows multi-outcome prediction, usually named multi-label classification multi-output regression outcomes numerical. Multi-outcome currently expect outcomes either numeric categorical.","code":""},{"path":"/reference/tabnet_fit.html","id":"threading","dir":"Reference","previous_headings":"","what":"Threading","title":"Tabnet model — tabnet_fit","text":"TabNet uses torch backend computation torch uses available threads default. can control number threads used torch :","code":"torch::torch_set_num_threads(1) torch::torch_set_num_interop_threads(1)"},{"path":"/reference/tabnet_fit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tabnet model — tabnet_fit","text":"","code":"if (FALSE) { # \\dontrun{ data(\"ames\", package = \"modeldata\") data(\"attrition\", package = \"modeldata\")  ## Single-outcome regression using formula specification fit <- tabnet_fit(Sale_Price ~ ., data = ames, epochs = 4)  ## Single-outcome classification using data-frame specification attrition_x <- attrition[ids,-which(names(attrition) == \"Attrition\")] fit <- tabnet_fit(attrition_x, attrition$Attrition, epochs = 4, verbose = TRUE)  ## Multi-outcome regression on `Sale_Price` and `Pool_Area` in `ames` dataset using formula, ames_fit <- tabnet_fit(Sale_Price + Pool_Area ~ ., data = ames, epochs = 4, valid_split = 0.2)  ## Multi-label classification on `Attrition` and `JobSatisfaction` in ## `attrition` dataset using recipe library(recipes) rec <- recipe(Attrition + JobSatisfaction ~ ., data = attrition) %>%   step_normalize(all_numeric(), -all_outcomes())  attrition_fit <- tabnet_fit(rec, data = attrition, epochs = 4, valid_split = 0.2)  ## Hierarchical classification on  `acme` data(acme, package = \"data.tree\")  acme_fit <- tabnet_fit(acme, epochs = 4, verbose = TRUE)  # Note: Model's number of epochs should be increased for publication-level results. } # }"},{"path":"/reference/tabnet_nn.html","id":null,"dir":"Reference","previous_headings":"","what":"TabNet Model Architecture — tabnet_nn","title":"TabNet Model Architecture — tabnet_nn","text":"nn_module representing TabNet architecture Attentive Interpretable Tabular Deep Learning.","code":""},{"path":"/reference/tabnet_nn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"TabNet Model Architecture — tabnet_nn","text":"","code":"tabnet_nn(   input_dim,   output_dim,   n_d = 8,   n_a = 8,   n_steps = 3,   gamma = 1.3,   cat_idxs = c(),   cat_dims = c(),   cat_emb_dim = 1,   n_independent = 2,   n_shared = 2,   epsilon = 1e-15,   virtual_batch_size = 128,   momentum = 0.02,   mask_type = \"sparsemax\",   mask_topk = NULL )"},{"path":"/reference/tabnet_nn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"TabNet Model Architecture — tabnet_nn","text":"input_dim Initial number features. output_dim Dimension network output. Examples : one regression, 2 binary classification etc.. Vector dimensions case multi-output. n_d Dimension prediction layer (usually 4 64). n_a Dimension attention layer (usually 4 64). n_steps Number successive steps network (usually 3 10). gamma Scaling factor attention updates (usually 1 2). cat_idxs Index categorical column dataset. cat_dims Number categories categorical column. cat_emb_dim Size embedding categorical features int, categorical features embedding size list int, every corresponding feature specific size. n_independent Number independent GLU layer GLU block encoder. n_shared Number shared GLU layer GLU block encoder. epsilon Avoid log(0), kept low. virtual_batch_size Batch size Ghost Batch Normalization. momentum Numerical value 0 1 used momentum batch norm. mask_type Either \"sparsemax\", \"entmax\" \"entmax15\": sparse masking function use. mask_topk mask top-k value k-sparsity selection mask sparsemax entmax15. defaults 1/4 last input_dim NULL. See entmax15 details.","code":""},{"path":"/reference/tabnet_non_tunable.html","id":null,"dir":"Reference","previous_headings":"","what":"Non-tunable parameters for the tabnet model — cat_emb_dim","title":"Non-tunable parameters for the tabnet model — cat_emb_dim","text":"Non-tunable parameters tabnet model","code":""},{"path":"/reference/tabnet_non_tunable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Non-tunable parameters for the tabnet model — cat_emb_dim","text":"","code":"cat_emb_dim(range = NULL, trans = NULL)  checkpoint_epochs(range = NULL, trans = NULL)  drop_last(range = NULL, trans = NULL)  encoder_activation(range = NULL, trans = NULL)  lr_scheduler(range = NULL, trans = NULL)  mlp_activation(range = NULL, trans = NULL)  mlp_hidden_multiplier(range = NULL, trans = NULL)  num_independent_decoder(range = NULL, trans = NULL)  num_shared_decoder(range = NULL, trans = NULL)  optimizer(range = NULL, trans = NULL)  penalty(range = NULL, trans = NULL)  verbose(range = NULL, trans = NULL)  virtual_batch_size(range = NULL, trans = NULL)"},{"path":"/reference/tabnet_non_tunable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Non-tunable parameters for the tabnet model — cat_emb_dim","text":"range unused trans unused","code":""},{"path":"/reference/tabnet_params.html","id":null,"dir":"Reference","previous_headings":"","what":"Parameters for the tabnet model — attention_width","title":"Parameters for the tabnet model — attention_width","text":"Parameters tabnet model","code":""},{"path":"/reference/tabnet_params.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parameters for the tabnet model — attention_width","text":"","code":"attention_width(range = c(8L, 64L), trans = NULL)  decision_width(range = c(8L, 64L), trans = NULL)  feature_reusage(range = c(1, 2), trans = NULL)  momentum(range = c(0.01, 0.4), trans = NULL)  mask_type(values = c(\"sparsemax\", \"entmax\"))  num_independent(range = c(1L, 5L), trans = NULL)  num_shared(range = c(1L, 5L), trans = NULL)  num_steps(range = c(3L, 10L), trans = NULL)"},{"path":"/reference/tabnet_params.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parameters for the tabnet model — attention_width","text":"range default range parameter value trans whether apply transformation parameter values possible values factor parameters functions used tune grid functions generate candidates.","code":""},{"path":"/reference/tabnet_params.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parameters for the tabnet model — attention_width","text":"dials parameter used tuning TabNet models.","code":""},{"path":"/reference/tabnet_params.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Parameters for the tabnet model — attention_width","text":"","code":"model <- tabnet(attention_width = tune(), feature_reusage = tune(),     momentum = tune(), penalty = tune(), rate_step_size = tune()) %>%     parsnip::set_mode(\"regression\") %>%     parsnip::set_engine(\"torch\")"},{"path":"/reference/tabnet_pretrain.html","id":null,"dir":"Reference","previous_headings":"","what":"Tabnet model — tabnet_pretrain","title":"Tabnet model — tabnet_pretrain","text":"Pretrain TabNet: Attentive Interpretable Tabular Learning model predictor data exclusively (unsupervised training).","code":""},{"path":"/reference/tabnet_pretrain.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tabnet model — tabnet_pretrain","text":"","code":"tabnet_pretrain(x, ...)  # Default S3 method tabnet_pretrain(x, ...)  # S3 method for class 'data.frame' tabnet_pretrain(   x,   y,   tabnet_model = NULL,   config = tabnet_config(),   ...,   from_epoch = NULL )  # S3 method for class 'formula' tabnet_pretrain(   formula,   data,   tabnet_model = NULL,   config = tabnet_config(),   ...,   from_epoch = NULL )  # S3 method for class 'recipe' tabnet_pretrain(   x,   data,   tabnet_model = NULL,   config = tabnet_config(),   ...,   from_epoch = NULL )  # S3 method for class 'Node' tabnet_pretrain(   x,   tabnet_model = NULL,   config = tabnet_config(),   ...,   from_epoch = NULL )"},{"path":"/reference/tabnet_pretrain.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tabnet model — tabnet_pretrain","text":"x Depending context: data frame predictors. matrix predictors. recipe specifying set preprocessing steps created recipes::recipe(). Node tree leaves left , attributes used predictors. predictor data standardized (e.g. centered scaled). model treats categorical predictors internally thus, need make treatment. model treats missing values internally thus, need make treatment. ... Model hyperparameters. hyperparameters set update set config argument. See tabnet_config() list possible hyperparameters. y (optional) x data frame matrix, y outcome tabnet_model pretrained tabnet_model object continue fitting . NULL (default) brand new model initialized. config set hyperparameters created using tabnet_config function. argument supplied, use default values tabnet_config(). from_epoch tabnet_model provided, restore network weights specific epoch. Default last available checkpoint restored model, last epoch -memory model. formula formula specifying outcome terms left-hand side, predictor terms right-hand side. data recipe formula used, data specified : data frame containing predictors outcome.","code":""},{"path":"/reference/tabnet_pretrain.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tabnet model — tabnet_pretrain","text":"TabNet model object. can used serialization, predictions, fitting.","code":""},{"path":"/reference/tabnet_pretrain.html","id":"outcome","dir":"Reference","previous_headings":"","what":"outcome","title":"Tabnet model — tabnet_pretrain","text":"Outcome value accepted consistent syntax tabnet_fit, design outcome, present, ignored pre-training.","code":""},{"path":"/reference/tabnet_pretrain.html","id":"pre-training-from-a-previous-model","dir":"Reference","previous_headings":"","what":"pre-training from a previous model","title":"Tabnet model — tabnet_pretrain","text":"providing parent tabnet_model parameter, model pretraining resumes model weights following epoch: last pretrained epoch model already torch context Last model checkpoint epoch model loaded file epoch related checkpoint matching preceding from_epoch value provided model pretraining metrics append top parent metrics returned TabNet model.","code":""},{"path":"/reference/tabnet_pretrain.html","id":"threading","dir":"Reference","previous_headings":"","what":"Threading","title":"Tabnet model — tabnet_pretrain","text":"TabNet uses torch backend computation torch uses available threads default. can control number threads used torch :","code":"torch::torch_set_num_threads(1) torch::torch_set_num_interop_threads(1)"},{"path":"/reference/tabnet_pretrain.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tabnet model — tabnet_pretrain","text":"","code":"data(\"ames\", package = \"modeldata\") pretrained <- tabnet_pretrain(Sale_Price ~ ., data = ames, epochs = 1)"},{"path":[]},{"path":"/news/index.html","id":"tabnet-080","dir":"Changelog","previous_headings":"","what":"tabnet 0.8.0","title":"tabnet 0.8.0","text":"CRAN release: 2026-01-31","code":""},{"path":"/news/index.html","id":"new-features-0-8-0","dir":"Changelog","previous_headings":"","what":"New features","title":"tabnet 0.8.0","text":"messaging now improved {cli} add optimal threshold support size new 1.5 alpha entmax15() sparsemax15() mask_types. Add optional mask_topk config parameter. (#180) optimizernow default torch_ignite_adam available. Result 30% faster pretraining fitting tasks (#178). add nn_aum_loss() function area Min(FPR,FNR)Min(FPR,FNR) optimization cases unbalanced binary classification (#178). add vignette imbalanced binary classification nn_aum_loss() (#178).","code":""},{"path":"/news/index.html","id":"bugfixes-0-8-0","dir":"Changelog","previous_headings":"","what":"Bugfixes","title":"tabnet 0.8.0","text":"config parameter now merge correctly torch loss torch optimizer generator. nn_unsupervised_loss() now proper loss function.","code":""},{"path":"/news/index.html","id":"tabnet-070","dir":"Changelog","previous_headings":"","what":"tabnet 0.7.0","title":"tabnet 0.7.0","text":"CRAN release: 2025-04-16","code":""},{"path":"/news/index.html","id":"bugfixes-0-7-0","dir":"Changelog","previous_headings":"","what":"Bugfixes","title":"tabnet 0.7.0","text":"Remove long-run example raising Note. fix tabet_pretrain failing value_error(\"convert data class: 'NULL'\") R 4.5 fix tabet_pretrain wrongly used instead tabnet_fit Missing data predictor vignette improve message related case_weights used predictors. improve function documentation consistency translation. fix “…” exported object ‘namespace:dials’” error using tune() tabnet parameters. (#160 @cphaarmeyer)","code":""},{"path":"/news/index.html","id":"tabnet-060","dir":"Changelog","previous_headings":"","what":"tabnet 0.6.0","title":"tabnet 0.6.0","text":"CRAN release: 2024-06-15","code":""},{"path":"/news/index.html","id":"new-features-0-6-0","dir":"Changelog","previous_headings":"","what":"New features","title":"tabnet 0.6.0","text":"parsnip models now allow transparently passing case weights workflows::add_case_weights() parameters (#151) parsnip models now support tabnet_model from_epoch parameters (#143)","code":""},{"path":"/news/index.html","id":"bugfixes-0-6-0","dir":"Changelog","previous_headings":"","what":"Bugfixes","title":"tabnet 0.6.0","text":"Adapt tune::finalize_workflow() test {parsnip} v1.2 breaking change. (#155) autoplot() now position “has_checkpoint” points correctly tabnet_fit() continuing previous training using tabnet_model =. (#150) Explicitely warn tabnet_model option used tabnet_pretrain() tasks. (#150)","code":""},{"path":"/news/index.html","id":"tabnet-050","dir":"Changelog","previous_headings":"","what":"tabnet 0.5.0","title":"tabnet 0.5.0","text":"CRAN release: 2023-12-05","code":""},{"path":"/news/index.html","id":"new-features-0-5-0","dir":"Changelog","previous_headings":"","what":"New features","title":"tabnet 0.5.0","text":"{tabnet} now allows hierarchical multi-label classification {data.tree} hierarchical Node dataset. (#126) tabnet_pretrain() now allows different GLU blocks GLU layers encoder decoder config() parameters num_idependant_decoder num_shared_decoder (#129) Add reduce_on_plateau option lr_scheduler tabnet_config() (@SvenVw, #120) use zeallot internally %<-% code readability (#133) add FR translation (#131)","code":""},{"path":"/news/index.html","id":"tabnet-040","dir":"Changelog","previous_headings":"","what":"tabnet 0.4.0","title":"tabnet 0.4.0","text":"CRAN release: 2023-05-11","code":""},{"path":"/news/index.html","id":"new-features-0-4-0","dir":"Changelog","previous_headings":"","what":"New features","title":"tabnet 0.4.0","text":"Add explicit legend autoplot.tabnet_fit() (#67) Improve unsupervised vignette content. (#67) tabnet_pretrain() now allows missing values predictors. (#68) tabnet_explain() now works tabnet_pretrain models. (#68) Allow missing-values values predictor unsupervised training. (#68) Improve performance random_obfuscator() torch_nn module. (#68) Add support early stopping (#69) tabnet_fit() predict() now allow missing values predictors. (#76) tabnet_config() now supports num_workers= parameters control parallel dataloading (#83) Add vignette missing data (#83) tabnet_config() now flag skip_importance skip calculating feature importance (@egillax, #91) Export document tabnet_nn Added min_grid.tabnet method tune (@cphaarmeyer, #107) Added tabnet_explain() method parsnip models (@cphaarmeyer, #108) tabnet_fit() predict() now allow multi-outcome, numeric factors mixed. (#118)","code":""},{"path":"/news/index.html","id":"bugfixes-0-4-0","dir":"Changelog","previous_headings":"","what":"Bugfixes","title":"tabnet 0.4.0","text":"tabnet_explain() now correctly handling missing values predictors. (#77) dataloader can now use num_workers>0 (#83) new default values batch_size virtual_batch_size improves performance mid-range devices. add default engine=\"torch\" tabnet parsnip model (#114) fix autoplot() warnings turned errors {ggplot2} v3.4 (#113)","code":""},{"path":"/news/index.html","id":"tabnet-030","dir":"Changelog","previous_headings":"","what":"tabnet 0.3.0","title":"tabnet 0.3.0","text":"CRAN release: 2021-10-11 Added update method tabnet models allow correct usage finalize_workflow (#60).","code":""},{"path":"/news/index.html","id":"tabnet-020","dir":"Changelog","previous_headings":"","what":"tabnet 0.2.0","title":"tabnet 0.2.0","text":"CRAN release: 2021-06-22","code":""},{"path":"/news/index.html","id":"new-features-0-2-0","dir":"Changelog","previous_headings":"","what":"New features","title":"tabnet 0.2.0","text":"Allow model fine-tuning passing pre-trained model tabnet_fit() (@cregouby, #26) Explicit error case missing values (@cregouby, #24) Better handling larger datasets running tabnet_explain(). Add tabnet_pretrain() unsupervised pretraining (@cregouby, #29) Add autoplot() model loss among epochs (@cregouby, #36) Added config argument fit() / pretrain() one can pass pre-made config list. (#42) tabnet_config(), new mask_type option entmax additional default sparsemax (@cmcmaster1, #48) tabnet_config(), loss now also takes function (@cregouby, #55)","code":""},{"path":"/news/index.html","id":"bugfixes-0-2-0","dir":"Changelog","previous_headings":"","what":"Bugfixes","title":"tabnet 0.2.0","text":"Fixed bug GPU training. (#22) Fixed memory leaks using custom autograd function. Batch predictions avoid OOM error.","code":""},{"path":"/news/index.html","id":"internal-improvements-0-2-0","dir":"Changelog","previous_headings":"","what":"Internal improvements","title":"tabnet 0.2.0","text":"Added GPU CI. (#22)","code":""},{"path":"/news/index.html","id":"tabnet-010","dir":"Changelog","previous_headings":"","what":"tabnet 0.1.0","title":"tabnet 0.1.0","text":"CRAN release: 2021-01-14 Added NEWS.md file track changes package.","code":""}]
