---
title: "Interpretabnet"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Interpretabnet}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  out.width = "100%", 
  out.height = "100%", 
  fig.width = 14
)
```

```{r setup}
library(tabnet)
library(dplyr)
library(purrr)
library(rsample)
library(yardstick)
library(ggplot2)
library(patchwork)
```

# Interprestability 

In this vignette, we will try to improve the workflow on Ames dataset debuted in the [Missing_data_predictors] vignette.

Interprestability score associated with `tabnet_explain()` results, will help us to select more stable models.  

Interprestability score is a metric for the stability of mask between models: score over 0.9 relates **very-high** stability, between 0.7 and 0.9 is **high** stability, between 0.5 and 0.7 is **moderate** and between 0.3 and 0.5 **low** stability of the interpretation on the model.


The {tabnet} implementation compares the explainability parameters between the last 5 model checkpoints. So it is up to you to make those last 5 checkpoints a good proxy of the model.

Let's experiment those on a pretraining scenario on the `ames` dataset : 

## Interprestability on ames_missing 

We will work here with the `ames_missing` dataset, transformation of the `ames` dataset done in vignette [Missing Data].

* Experiment scenario
Let's train 3 different models on ames_missing dataset with 2 flavors of parameter, tabnet and interpretabnet.
We will train for the time needed for convergence via early-stopping, and continue the training for 5 epochs with a record of each checkpoints. 
We will thus have a way to measure the interprestability and compare stability between thos 6 models

### 3 classical TabNet models
```{r, 3 classical models }
data("ames_missing", package = "tabnet")
cat_emb_dim <- map_dbl(ames_missing %>% select_if(is.factor), ~log2(nlevels(.x)) %>% round)

tabnet_config <- tabnet_config( num_steps = 3, attention_width = 8, num_shared = 2,
                                num_independent = 2, cat_emb_dim = cat_emb_dim, verbose = FALSE,
                                early_stopping_patience = 12L, early_stopping_tolerance = 0.01,
                                valid_split = 0.2)

train_tabnet <- map(1:3, ~tabnet_fit(Sale_Price ~., data = ames_missing,
                                     epochs = 100, checkpoint_epoch = 101,
                                     config = tabnet_config, learn_rate = 5e-2),
                    .progress = TRUE)
```

### 3 InterpreTabNet models

The difference of interpretabnet models is 
- the presence of a MLP adaptation layer in between steps
- the encoder layer use a Multibranch Weighted Linear-Unit implemented as `nn_mb_wlu()`
- the mask type switch to `entmax`

You don't have to know them, as the handy `interpretabnet_config()` function change them all in once.

For the training loop, you will realize that Interpretabnet models need many more epoch to train due to the MLP network.

```{r, 3 interpretabnet models }
interpretabnet_config <- interpretabnet_config( num_steps = 3, attention_width = 8, num_shared = 2,
                                num_independent = 2, cat_emb_dim = cat_emb_dim, verbose = FALSE,
                                early_stopping_patience = 12L, early_stopping_tolerance = 0.01,
                                valid_split = 0.2)


train_tabnet <- c(train_tabnet,
                  map(1:3, ~tabnet_fit(Sale_Price ~., data = ames_missing,
                                     epochs = 150, checkpoint_epoch = 151,
                                     config = interpretabnet_config, learn_rate = 5e-2),
                    .progress = TRUE)
)

```

### Adding 5 checkpoont to each model

With a small learning-rate, we will extend the fitted model for 5 epochs in order to measure the Inteprestability.

```{r, fig.width= 10}
autoplot(train_tabnet[[1]]) + 
  autoplot(train_tabnet[[2]]) + 
  autoplot(train_tabnet[[3]]) + 
  autoplot(train_tabnet[[4]]) + 
  autoplot(train_tabnet[[5]])  +
  autoplot(train_tabnet[[6]])  +
  plot_layout(axes = "collect", guides = "collect")
  
```

```{r, trailing 5 checkpoint}

models_checkpointed <- map(train_tabnet, ~tabnet_fit(Sale_Price ~., data = ames_missing,
                                     tabnet_model = .x, epochs = 6, valid_split = 0.2,
                                     checkpoint_epoch = 1, learn_rate = 1e-2),
                           .progress = TRUE)

```




Let's first have a close look at their training convergence plot

```{r, fig.width= 10}
autoplot(models_checkpointed[[1]]) + 
  autoplot(models_checkpointed[[2]]) + 
  autoplot(models_checkpointed[[3]]) + 
  autoplot(models_checkpointed[[4]]) + 
  autoplot(models_checkpointed[[5]])  +
  autoplot(models_checkpointed[[6]])  +
  plot_layout(axes = "collect", guides = "collect")
  
```

### evolution of the Interpretabnet score along the checkpoints


```{r, explain pretraining}
explain_lst <- map(models_checkpointed, tabnet_explain, new_data = ames_missing)
interprestability <- map_dbl(explain_lst, "interprestability")
interprestability
```

### plot the 6 different models
```{r, plot explain pretrain, fig.width= 13}

autoplot(explain_lst[[1]], quantile = 0.99) + 
  autoplot(explain_lst[[2]], quantile = 0.99) + 
  autoplot(explain_lst[[3]], quantile = 0.99) + 
  autoplot(explain_lst[[4]], quantile = 0.99) + 
  autoplot(explain_lst[[5]], quantile = 0.99)  +
  autoplot(explain_lst[[6]], quantile = 0.99)  +
  plot_layout(axes = "collect", guides = "collect")
  
```
## Interprestability on ames, supervised learning

### Dataset preparation

```{r, 6 models dataset}
ames_split <- initial_split(ames_missing, strata = Sale_Price, prop = 0.8)
ames_train <- training(ames_split)
ames_test <- testing(ames_split)
```

