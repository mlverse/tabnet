#' Self-supervised learning loss
#'
#' Creates a criterion that measures the Autoassociative self-supervised learning loss between each
#' element in the input \eqn{y_pred} and target \eqn{embedded_x} on the values masked by \eqn{obfuscation_mask}. 
#' 
#' @noRd
nn_unsupervised_loss <- torch::nn_module(
  "nn_unsupervised_loss",
  inherit = torch::nn_cross_entropy_loss,
  
  initialize = function(eps = 1e-9){
    super$initialize()
    self$eps = eps
  },
  
  forward = function(y_pred, embedded_x, obfuscation_mask){
    errors <- y_pred - embedded_x
    reconstruction_errors <- torch::torch_mul(errors, obfuscation_mask) ^ 2
    batch_stds <- torch::torch_std(embedded_x, dim = 1) ^ 2 + self$eps
    
    # compute the number of obfuscated variables to reconstruct
    nb_reconstructed_variables <- torch::torch_sum(obfuscation_mask, dim = 2)
    
    # take the mean of the reconstructed variable errors
    features_loss <- torch::torch_matmul(reconstruction_errors, 1 / batch_stds) / (nb_reconstructed_variables +  self$eps)
    loss <- torch::torch_mean(features_loss, dim = 1)
    loss
  }
)


#' AUM loss
#'
#' Creates a criterion that measures the Area under the \eqn{Min(FPR, FNR)} (AUM) between each
#' element in the input \eqn{pred_tensor} and target \eqn{label_tensor}. 
#' 
#' This is used for measuring the error of a binary reconstruction within highly unbalanced dataset, 
#' where the goal is optimizing the ROC curve. Note that the targets \eqn{label_tensor} should be factor
#' level of the binary outcome, i.e. with values `1L` and `2L`.
#'
#' @examples
#' loss <- nn_aum_loss()
#' input <- torch::torch_randn(4, 6, requires_grad = TRUE)
#' target <- input > 1.5
#' output <- loss(input, target)
#' output$backward()
#' @export
nn_aum_loss <- torch::nn_module(
  "nn_aum_loss",
  inherit = torch::nn_mse_loss,
  initialize = function(){
    super$initialize()
  },
  forward = function(pred_tensor, label_tensor){
    # thanks to https://github.com/tdhock/2023-res-baz-az/blob/main/HOCKING-slides-TRUG.R
    is_positive = label_tensor == label_tensor$max()
    is_negative = is_positive$bitwise_not()
    # manage case when prediction error is null (prevent division by 0)
    if(as.logical(torch::torch_sum(is_positive) == 0) || as.logical(torch::torch_sum(is_negative) == 0)){
      return(torch::torch_sum(pred_tensor*0))
    }
    # nominal case
    fn_diff = torch::torch_where(is_positive, -1, 0)
    fp_diff = torch::torch_where(is_positive, 0, 1)
    thresh_tensor = -pred_tensor[,1] # pred tensor is [prediction, class_probability]. wee keep only prediction
    fp_denom = torch::torch_sum(is_negative) #or 1 for AUM based on count instead of rate
    fn_denom = torch::torch_sum(is_positive) #or 1 for AUM based on count instead of rate
    sorted_indices = torch::torch_argsort(thresh_tensor)
    sorted_fp_cum = fp_diff[sorted_indices]$cumsum(dim=1)/fp_denom
    sorted_fn_cum = -fn_diff[sorted_indices]$flip(1)$cumsum(dim=1)$flip(1)/fn_denom
    sorted_thresh = thresh_tensor[sorted_indices]
    sorted_is_diff = sorted_thresh$diff() != 0
    sorted_fp_end = torch::torch_cat(c(sorted_is_diff, torch::torch_tensor(TRUE)))
    sorted_fn_end = torch::torch_cat(c(torch::torch_tensor(TRUE), sorted_is_diff))
    uniq_thresh = sorted_thresh[sorted_fp_end]
    uniq_fp_after = sorted_fp_cum[sorted_fp_end]
    uniq_fn_before = sorted_fn_cum[sorted_fn_end]
    FPR = torch::torch_cat(c(torch::torch_tensor(0.0), uniq_fp_after))
    FNR = torch::torch_cat(c(uniq_fn_before, torch::torch_tensor(0.0)))
    roc = list(
      FPR = FPR,
      FNR = FNR,
      TPR = 1 - FNR,
      "min(FPR,FNR)"=torch::torch_minimum(FPR, FNR),
      min_constant=torch::torch_cat(c(torch::torch_tensor(-Inf), uniq_thresh)),
      max_constant=torch::torch_cat(c(uniq_thresh, torch::torch_tensor(Inf))))
    min_FPR_FNR = roc[["min(FPR,FNR)"]][2:-2]
    constant_diff = roc$min_constant[2:N]$diff()
    torch::torch_sum(min_FPR_FNR * constant_diff)
    
  }
)
