% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/activation.R
\name{nn_mb_wlu}
\alias{nn_mb_wlu}
\alias{nnf_mb_wlu}
\title{Multi-branch Weighted Linear Unit (MB-wLU) nn module.}
\usage{
nn_mb_wlu(alpha = 0.6, beta = 0.2, gamma = 0.2, init = 0.25)

nnf_mb_wlu(input, alpha = 0.6, beta = 0.2, gamma = 0.2, init = 0.25)
}
\arguments{
\item{alpha}{(float) the weight of ELU activation component.}

\item{beta}{(float) the weight of PReLU activation component.}

\item{gamma}{(float) the weight of SiLU activation component.}

\item{init}{(float): the initial value of \eqn{a} of PReLU. Default: 0.25.}

\item{input}{(N,*) tensor, where * means, any number of additional
dimensions}
}
\value{
an activation function computing
\eqn{\mathbf{MBwLU(input) = \alpha \times ELU(input) + \beta \times PReLU(input) + \gamma \times SiLU(input)}}
}
\description{
Multi-branch Weighted Linear Unit (MB-wLU) nn module.

Applies the Multi-branch Weighted Linear Unit (MB-wLU) function, element_wise.
See \code{\link[=nn_mb_wlu]{nn_mb_wlu()}} for more information.
}
\examples{
\dontshow{if (torch::torch_is_installed()) (if (getRversion() >= "3.4") withAutoprint else force)(\{ # examplesIf}
x <- torch::torch_randn(2, 2)
my_mb_wlu <- nn_mb_wlu(alpha = 0.6, beta = 0.2, gamma = 0.2)
default_mb_wlu <- nn_mb_wlu()
y <- my_mb_wlu(x)
z <- default_mb_wlu(x)
torch::torch_equal(y, z)
\dontshow{\}) # examplesIf}
}
\seealso{
\code{\link[=nn_mb_wlu]{nn_mb_wlu()}}.
}
