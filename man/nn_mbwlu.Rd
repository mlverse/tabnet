% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/activation.R
\name{nn_mbwlu}
\alias{nn_mbwlu}
\alias{nnf_mbwlu}
\title{Multibranch Weighted Linear Unit (MB-WLU) nn module.}
\usage{
nn_mbwlu(alpha = 0.6, beta = 0.2, gamma = 0.2, init = 0.25)

nnf_mbwlu(input, alpha, beta, gamma, init)
}
\arguments{
\item{alpha}{(float) the weight of ELU activation component.}

\item{beta}{(float) the weight of PRELU activation component.}

\item{gamma}{(float) the weight of SILU activation component.}

\item{init}{(float): the initial value of \eqn{a} of PRELU. Default: 0.25.}

\item{input}{(N,*) tensor, where * means, any number of additional
dimensions}
}
\description{
Multibranch Weighted Linear Unit (MB-WLU) nn module.

Applies the Multibranch Weighted Linear Unit (MB-WLU) function, element_wise.
See \code{\link[=nn_mbwlu]{nn_mbwlu()}} for more information.
}
\examples{
\dontshow{if (torch::torch_is_installed()) (if (getRversion() >= "3.4") withAutoprint else force)(\{ # examplesIf}
x <- torch::torch_randn(2, 2)
my_mbwlu <- nn_mbwlu(alpha = 0.6, beta = 0.2, gamma = 0.2)
mbwlu <- nn_mbwlu()
y <- my_mbwlu(x)
z <- mbwlu(x)
torch::torch_equal(y, z)
\dontshow{\}) # examplesIf}
}
\seealso{
\code{\link[=nn_mbwlu]{nn_mbwlu()}}.
}
