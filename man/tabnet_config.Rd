% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/model.R
\name{tabnet_config}
\alias{tabnet_config}
\alias{interpretabnet_config}
\title{Configuration for TabNet models}
\usage{
tabnet_config(
  cat_emb_dim = 1L,
  decision_width = 8L,
  attention_width = 8L,
  num_steps = 3L,
  mask_type = "sparsemax",
  mlp_hidden_multiplier = NULL,
  mlp_activation = nn_relu(),
  encoder_activation = nn_relu(),
  num_independent = 2L,
  num_shared = 2L,
  num_independent_decoder = 1L,
  num_shared_decoder = 1L,
  penalty = 0.001,
  feature_reusage = 1.3,
  momentum = 0.02,
  epochs = 5L,
  batch_size = 1024^2,
  virtual_batch_size = 256^2,
  learn_rate = 0.02,
  optimizer = "adam",
  valid_split = 0,
  loss = "auto",
  clip_value = NULL,
  drop_last = FALSE,
  lr_scheduler = NULL,
  lr_decay = 0.1,
  step_size = 30,
  checkpoint_epochs = 10,
  pretraining_ratio = 0.5,
  verbose = TRUE,
  device = "auto",
  importance_sample_size = 1e+05,
  early_stopping_monitor = "auto",
  early_stopping_tolerance = 0,
  early_stopping_patience = 0L,
  num_workers = 0L,
  skip_importance = FALSE
)

interpretabnet_config(
  mask_type = "entmax",
  mlp_hidden_multiplier = c(4, 2),
  mlp_activation = NULL,
  encoder_activation = nn_mb_wlu(),
  ...
)
}
\arguments{
\item{cat_emb_dim}{Size of the embedding of categorical features. If a single interger, all categorical
features will have same embedding size, if a list of integer, every corresponding feature will have
specific embedding size.}

\item{decision_width}{Width of the decision prediction layer or \eqn{\mathbf{n_d}}. Bigger values gives
more capacity to the model with the risk of overfitting. Values typically
range from 8 to 64.}

\item{attention_width}{Width of the attention embedding for each mask or \eqn{\mathbf{n_a}}. According to
the paper \eqn{\mathbf{n_a = n_d}} is usually a good choice. (default=8)}

\item{num_steps}{Number of steps in the architecture
(usually between 3 and 10)}

\item{mask_type}{Final layer of feature selector in the attentive_transformer
block, either \code{"sparsemax"} or \code{"entmax"}.}

\item{mlp_hidden_multiplier}{NULL (tabnet) or a vector of 2 values being the size of the 2 hidden layers
of the MLP block (InterpreTabnet).}

\item{mlp_activation}{the torch nn_ activation function for the MLP-attentive-transformer part.
If \code{NULL} then \code{nn_relu()} will be used. (InterpreTabnet).}

\item{encoder_activation}{the torch nn_ activation function for the encoder part.
If \code{NULL} then \code{nn_relu()} will be used. (InterpreTabnet).}

\item{num_independent}{Number of independent Gated Linear Units layers at each step of the encoder.
Usual values range from 1 to 5.}

\item{num_shared}{Number of shared Gated Linear Units at each step of the encoder. Usual values
at each step of the decoder. range from 1 to 5}

\item{num_independent_decoder}{For pretraining, number of independent Gated Linear Units layers
Usual values range from 1 to 5.}

\item{num_shared_decoder}{For pretraining, number of shared Gated Linear Units at each step of the
decoder. Usual values range from 1 to 5.}

\item{penalty}{This is the extra sparsity loss coefficient as proposed
in the original paper as \eqn{\mathbf{\lambda}}. The bigger this coefficient is, the sparser
your model will be in terms of feature selection. Depending on the difficulty of
your problem, reducing this value could help (default 1e-3).}

\item{feature_reusage}{This is the \eqn{\mathbf{\gamma}} coefficient for feature re-usage
in the masks. A value close to 1 will make mask selection least correlated between layers.
Values range from 1.0 to 2.0.}

\item{momentum}{Momentum for batch normalization, typically ranges from 0.01
to 0.4 (default 0.02)}

\item{epochs}{Number of training epochs.}

\item{batch_size}{Number of examples per batch, large batch sizes are
recommended. (default \eqn{\mathbf{1024^2}})}

\item{virtual_batch_size}{Size of the mini batches used for
"Ghost Batch Normalization" (default \eqn{\mathbf{256^2}})}

\item{learn_rate}{initial learning rate for the optimizer.}

\item{optimizer}{the optimization method. currently only \code{"adam"} is supported,
you can also pass any torch optimizer function.}

\item{valid_split}{(\verb{[0, 1)}) The fraction of the dataset used for validation.
(default = 0 means no split)}

\item{loss}{(character or function) Loss function for training (default to mse
for regression and cross entropy for classification)}

\item{clip_value}{If a float is given this will clip the gradient at
clip_value. (default \code{NULL} do not clip)}

\item{drop_last}{Whether to drop last batch if not complete during
training}

\item{lr_scheduler}{if \code{NULL}, no learning rate decay is used. If "step"
decays the learning rate by \code{lr_decay} every \code{step_size} epochs. If "reduce_on_plateau"
decays the learning rate by \code{lr_decay} when no improvement after \code{step_size} epochs.
It can also be a \link[torch:lr_scheduler]{torch::lr_scheduler} function that only takes the optimizer
as parameter. The \code{step} method is called once per epoch.}

\item{lr_decay}{multiplies the initial learning rate by \code{lr_decay} every
\code{step_size} epochs. Unused if \code{lr_scheduler} is a \code{torch::lr_scheduler}
or \code{NULL}.}

\item{step_size}{the learning rate scheduler step size. Unused if
\code{lr_scheduler} is a \code{torch::lr_scheduler} or \code{NULL}.}

\item{checkpoint_epochs}{checkpoint model weights and architecture every
\code{checkpoint_epochs}. (default 10). This may cause large memory usage.
Use \code{0} to disable checkpoints.}

\item{pretraining_ratio}{Ratio of features to mask for reconstruction during
pretraining.  Ranges from 0 to 1 (default 0.5)}

\item{verbose}{(logical) Whether to print progress and loss values during
training.}

\item{device}{the device to use for training. "cpu" or "cuda". The default ("auto")
uses  to "cuda" if it's available, otherwise uses "cpu".}

\item{importance_sample_size}{sample of the dataset to compute importance metrics.
If the dataset is larger than \eqn{\mathbf{10^5}} observations, importance will be computed on a
sample of size \eqn{\mathbf{10^5}} and display a warning.}

\item{early_stopping_monitor}{Metric to monitor for early_stopping. One of \code{"valid_loss"}, \code{"train_loss"} or \code{"auto"}
\code{auto} uses validation loss is there is a validation split, training loss else. (defaults to \code{"auto"}).}

\item{early_stopping_tolerance}{Minimum relative improvement to reset the patience counter.
0.01 for 1\% tolerance (default 0)}

\item{early_stopping_patience}{Number of epochs without improving until stopping training. (default 5)}

\item{num_workers}{(int, optional): how many subprocesses to use for data
loading. 0 means that the data will be loaded in the main process.
(default: \code{0})}

\item{skip_importance}{should feature importance calculation be skipped (default \code{FALSE})}
}
\value{
A named list with all parameters of the TabNet implementation.
}
\description{
This change few default values of \code{tabnet_config()} to turn model into InterpreTabnet
}
\section{Network design}{


The first 12 parameters \code{cat_emb_dim}, \code{decision_width}, \code{attention_width}, \code{num_steps},
\code{mask_type}, \code{mlp_hidden_multiplier}, \code{mlp_activation}, \code{encoder_activation}, \code{num_independent},
\code{num_shared}, \code{num_independent_decoder}, \code{num_shared_decoder} will form the Tabnet network
design through shapping each TabNet sub-component.
}

\section{Network hyper-parameters}{


The next 3 parameters \code{penalty}, \code{feature_reusage}, \code{momentum} are numerical hyper-parameter
that you may tune to adapt the model to your dataset characteristics
}

\section{Model training}{


The last 22 parameters control the training loop of the model.
}

